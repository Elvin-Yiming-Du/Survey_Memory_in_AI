# Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions

<div align="center">
 <p align="center">

<a href="https://arxiv.org/abs/2505.00675">üìù Paper</a> | <a href="#1-Survey-Papers">üìÑ List</a> | <a href="https://www.notion.so/Huanxuan-Liao-s-Blog-6518cf95f0d54858829b042588ff88bb">üìö Notions</a>

 </p>
</div>
<div align="center">


[![License](https://img.shields.io/github/license/Elvin-Yiming-Du/Survey_Memory_in_AI)](https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI/blob/main/LICENSE)
<!-- ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg) -->
[![commit](https://img.shields.io/github/last-commit/Elvin-Yiming-Du/Survey_Memory_in_AI?color=blue)](https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI/commits/main)
[![PR](https://img.shields.io/badge/PRs-Welcome-red)](https://github.com/Xnhyacinth/Long_Text_Modeling_Papers/pulls)
[![GitHub Repo stars](https://img.shields.io/github/stars/Elvin-Yiming-Du/Survey_Memory_in_AI)](https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI)

</div>

This repository introduce a comprehensive paper list, datasets, methods and tools for memory research.

ü§ùü§ù Thanks for all the great contributors on GitHub!

If you find our repository and survey useful for your research, please consider citing the following paper:

```bibtex
@article{du2025rethinking,
  title={Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions},
  author={Du, Yiming and Huang, Wenyu and Zheng, Danna and Wang, Zhaowei and Montella, Sebastien and Lapata, Mirella and Wong, Kam-Fai and Pan, Jeff Z.},
  journal={arXiv preprint arXiv:2505.00675},
  year={2025},
  url={https://arxiv.org/abs/2505.00675}
}
```

## Contents

- [Memory in AI Papers, Dataset, Methods and Tools](#rethinking-memory-in-ai-taxonomy-operations-topics-and-future-directions)
- [üì¢ News](#-news)
    - [üìÖ Weekly Papers](#weekly-papers)
    - [üóì Monthly Papers](#monthly-papers)
- [Memory Taxonomy](#memory-taxonomy)
- [Memory Operations](#memory-operations)
- [üìú Papers](#-papers)
    - [1. Survey Papers](#1-survey-papers)
    - [2. Memory Topics](#4-memory-topics)
        - [2.1 Long Term Memory](#41-long-term-memory)
        - [2.2 Long Context Memory](#42-long-context-memory)
        - [2.3 Parametric Memory](#43-parametric-memory)
        - [2.4 Multi-source Memory](#44-multi-source-memory)
- [üìä Datasets](#-datasets)
- [üß† Methods](#-methods)
- [‚öôÔ∏è Tools](#-tools)
- [üåû Future Directions](#-future-directions)
- [üôè Acknowledgements](#acknowledgements)


### Memory Taxonomy
![Memory Taxonomy](./framework.png)

### Memory Operations
![Memory Operations](./operation_to_topics.png)

## üì¢ News

### Week Papers

## üìú Papers

### 1. Survey Papers
- 
1. [**A Survey on the Memory Mechanism of Large Language Model based Agents.**](https://arxiv.org/abs/2404.13501) _Zhang, Zeyu and Bo, Xiaohe and Ma, Chen and Li, Rui and Chen, Xu and Dai, Quanyu and Zhu, Jieming and Dong, Zhenhua and Wen, Ji-Rong._ Arxiv 2024.

2.[**Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey**](https://arxiv.org/abs/2311.12351) _Yunpeng Huang, Jingwei Xu, Junyu Lai, Zixu Jiang, Taolue Chen, Zenan Li, Yuan Yao, Xiaoxing Ma, Lijuan Yang, Hao Chen, Shupeng Li, Penghao Zhao._ Arxiv 2024.

3.[**Knowledge Editing for Large Language Models: A Survey**](https://arxiv.org/abs/2310.16218) _Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, Jundong Li._ Arxiv 2024.

4.[**Knowledge Conflicts for LLMs: A Survey**](https://arxiv.org/abs/2403.08319) _Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu._ Arxiv 2024.

5.[**A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models**](https://www.arxiv.org/abs/2503.01854) _Jiahui Geng, Qing Li, Herbert Woisetschlaeger, Zongxiong Chen, Yuxia Wang, Preslav Nakov, Hans-Arno Jacobsen, Fakhri Karray._ Arxiv 2025.

6.[**A Survey of Personalized Large Language Models: Progress and Future Directions**](https://arxiv.org/abs/2502.11528) _Jiahong Liu, Zexuan Qiu, Zhongyang Li, Quanyu Dai, Jieming Zhu, Minda Hu, Menglin Yang, Irwin King._ Arxiv 2025.

7.[**Human-inspired Perspectives: A Survey on AI Long-term Memory**](https://arxiv.org/abs/2411.00489) _Zihong He, Weizhe Lin, Hao Zheng, Fan Zhang, Matt W. Jones, Laurence Aitchison, Xuhai Xu, Miao Liu, Per Ola Kristensson, Junxiao Shen._ Arxiv 2025.

8.[**Cognitive Memory in Large Language Models**](https://arxiv.org/abs/2504.02441) _Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu._ Arxiv 2025.

9.[**A Comprehensive Survey on Long Context Language Modeling**](https://arxiv.org/abs/2503.17407)_Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, Yuanxing Zhang, Zhuo Chen, Hangyu Guo, Shilong Li, Ziqiang Liu, Yong Shan, Yifan Song, Jiayi Tian, Wenhao Wu, Zhejian Zhou, Ruijie Zhu, Junlan Feng, Yang Gao, Shizhu He, Zhoujun Li, Tianyu Liu, Fanyu Meng, Wenbo Su, Yingshui Tan, Zili Wang, Jian Yang, Wei Ye, Bo Zheng, Wangchunshu Zhou, Wenhao Huang, Sujian Li, Zhaoxiang Zhang_ Arxiv 2025.

### 2. Memory Topics

#### 2.1 Long Term Memory
- 
1.[**Evaluating Very Long-Term Conversational Memory of LLM Agents**](https://arxiv.org/abs/2402.17753)_Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang_ Arxiv 2024.

2.[**MemoryBank: Enhancing Large Language Models with Long-Term Memory**](https://arxiv.org/abs/2305.10250)_Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang_ Arxiv 2024.

3.[**A-MEM: Agentic Memory for LLM Agents**](https://arxiv.org/abs/2502.12110)_Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang_ Arxiv 2024.

4.[**Beyond Goldfish Memory: Long-Term Open-Domain Conversation**](https://arxiv.org/abs/2107.07567) _Jing Xu, Arthur Szlam, Jason Weston._ Arxiv 2021.

5.[**Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks**](https://arxiv.org/abs/2408.03615) _Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Shizhu He._ Arxiv 2024.

6.[**HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models**](https://arxiv.org/abs/2405.14831) _Yu Su, Yujia Xie, Yujie Qian, Yifan Hou, Xinyan Wang, Yiming Yang, Xiang Ren._ Arxiv 2024.

7.[**Long Time No See! Open-Domain Conversation with Long-Term Persona Memory**](https://arxiv.org/abs/2203.05797) _Jing Xu, Zhengyu Niu, Wenquan Wu, Hua Wu, Haifeng Wang, Zebin Gou, Shuo Wang._ Arxiv 2022.

8.[**LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory**](https://arxiv.org/abs/2410.10813) _Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu._ Arxiv 2024.

9.[**"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents**](https://arxiv.org/abs/2404.00573) _Yu Hou, Tamoto Yuta, Mayur Tikundi._ Arxiv 2024.

10.[**Keep Me Updated! Memory Management in Long-term Conversations**](https://arxiv.org/abs/2210.08750) _Sanghwan Bae, Donghyun Kwak, Soyoung Kang, Min Young Lee, Sungdong Kim, Yuin Jeong, Hyeri Kim, Sang-Woo Lee, Woomyoung Park, Nako Sung._ Arxiv 2022.

11.[**MoT: Memory-of-Thought Enables ChatGPT to Self-Improve**](https://arxiv.org/abs/2305.05181) _Sureman Lee, Yujie Qian, Yujia Xie, Yifan Hou, Xinyan Wang, Yiming Yang, Xiang Ren._ Arxiv 2023.

12.[**Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models**](https://arxiv.org/abs/2308.15022) _Qingxue Wang, Ling Ding, Yaran Cao, Zhilang Tan, Shi Wang, Dacheng Tao, Liu Qiu._ Arxiv 2023.

13.[**Mr.Steve: Instruction-Following Agents in Minecraft with What-Where-When Memory**](https://arxiv.org/abs/2411.06736) _Yu Hou, Tamoto Yuta, Mayur Tikundi._ Arxiv 2024.

14.[**LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination**](https://arxiv.org/abs/2309.11696) _Yuwei Zhang, Yifan Hou, Xinyan Wang, Yiming Yang, Xiang Ren._ Arxiv 2023.

15.[**SCM: Enhancing Large Language Model with Self-Controlled Memory Framework**](https://arxiv.org/abs/2304.13343) _Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li._ Arxiv 2023.

16.[**Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement**](https://arxiv.org/abs/2204.13074) _Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Clark._ Arxiv 2022.

17.[**StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization**](https://arxiv.org/abs/2311.14495) _Shida Wang, Qianxiao Li._ Arxiv 2023.

18.[**Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs**](https://arxiv.org/abs/2409.19401) _Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, Wei Shi._ Arxiv 2024.

19.[**Learning to Repair: Repairing Model Output Errors after Deployment Using a Dynamic Memory of Feedback**](https://arxiv.org/abs/2112.09737) _Niket Tandon, Aman Madaan, Peter Clark, Yiming Yang._ Arxiv 2021.

20.[**Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations**](https://arxiv.org/abs/2402.11975) _Nuo Chen, Hongguang Li, Jia Li, Yuxuan Li, Wei Wu._ Arxiv 2024.

21.[**Towards Verifiable Text Generation with Evolving Memory and Self-Reflection**](https://arxiv.org/abs/2312.09075) _Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xiaochi Wei, Shuaiqiang Wang, Yan Zhang, Dawei Yin._ Arxiv 2023.

22.[**There Are a Thousand Hamlets in a Thousand People's Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory**](https://arxiv.org/abs/2204.02624) _Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, Rui Yan._ Arxiv 2022.

23.[**PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering**](https://arxiv.org/abs/2402.16288) _Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, Kam-Fai Wong._ Arxiv 2024.

24.[**A Cooperative Memory Network for Personalized Task-oriented Dialogue Systems with Incomplete User Profiles**](https://arxiv.org/abs/2102.08322) _Jiahuan Pei, Pengjie Ren, Maarten de Rijke._ Arxiv 2021.

25.[**An Iterative Associative Memory Model for Empathetic Response Generation**](https://arxiv.org/abs/2402.17959) _Zhou Yang, Zhaochun Ren, Yufeng Wang, Haizhou Sun, Chao Chen, Xiaofei Zhu, Xiangwen Liao._ Arxiv 2024.

26.[**Towards Lifelong Dialogue Agents via Timeline-based Memory Management**](https://arxiv.org/abs/2406.10996) _Kai Tzu-iunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seung-won Hwang, Dongha Lee, Jinyoung Yeo._ Arxiv 2025.

27.[**COCOA: CBT-based Conversational Counseling Agent Using Memory Specialized in Cognitive Distortions and Dynamic Prompt**](https://arxiv.org/abs/2402.17546) _Suyeon Lee, Jieun Kang, Harim Kim, Kyoung-Mee Chung, Dongha Lee, Jinyoung Yeo._ Arxiv 2024.

28.[**Mixed-Session Conversation with Egocentric Memory**](https://arxiv.org/abs/2410.02503) _Jihyoung Jang, Taeyoung Kim, Hyounghun Kim._ Arxiv 2024.

29.[**FragRel: Exploiting Fragment-level Relations in the External Memory of Large Language Models**](https://arxiv.org/abs/2406.03092) _Xihang Yue, Linchao Zhu, Yi Yang._ Arxiv 2024.

30.[**LDM¬≤: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement**](https://arxiv.org/abs/2312.08402) _Xingjin Wang, Linjing Li, Dongfeng Zeng._ Arxiv 2023.

31.[**NarrativeXL: A Large-scale Dataset For Long-Term Memory Models**](https://arxiv.org/abs/2305.13877) _Arseny Moskvichev, Ky-Vinh Mai._ Arxiv 2023.

32.[**Extractive Medical Entity Disambiguation with Memory Mechanism and Memorized Entity Information**](https://aclanthology.org/2024.findings-emnlp.810/) _Guobiao Zhang, Xueping Peng, Tao Shen, Guodong Long, Jiasheng Si, Libo Qin, Wenpeng Lu._ Arxiv 2024.

33.[**Zep: A Temporal Knowledge Graph Architecture for Agent Memory**](https://arxiv.org/abs/2501.13956) _Preston Rasmussen, Daniel Chalef._ Arxiv 2025.

34.[**Ever-Evolving Memory by Blending and Refining the Past**](https://arxiv.org/abs/2403.04787) _Seo Hyun Kim, Keummin Ka, Yohan Jo, Seung-won Hwang, Dongha Lee, Jinyoung Yeo._ Arxiv 2024.

35.[**Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control**](https://arxiv.org/abs/2306.07863) _Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An_ Arxiv 2024.

#### 2.2 Long Context Memory
- 
1.[**Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks**](https://arxiv.org/abs/1503.00075) _Kai Sheng Tai, Richard Socher, Christopher D. Manning._ Arxiv 2015.

2.[**Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading**](https://arxiv.org/abs/2310.05029) _Howard Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz._ Arxiv 2023.

3.[**CaM: Cache Merging for Memory-efficient LLMs Inference**](https://proceedings.mlr.press/v235/zhang24n.html) _Yuxin Zhang, Yuxuan Du, Gen Luo, Yunshan Zhong, Zhenyu Zhang, Shiwei Liu, Rongrong Ji._ Arxiv 2024.

4.[**Recurrent Memory Transformer**](https://arxiv.org/abs/2207.06881) _Aydar Bulatov, Sergey I. Nikolenko, Artem Babenko._ Arxiv 2022.

5.[**InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory**](https://arxiv.org/abs/2402.04617) _Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun._ Arxiv 2024.

6.[**A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts**](https://arxiv.org/abs/2402.09727) _Xinyun Chen, Hiroki Furuta, John Canny, Ian Tenney._ Arxiv 2024.

7.[**Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control**](https://arxiv.org/abs/2306.07863) _Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An._ Arxiv 2023.

8.[**Mastering Memory Tasks with World Models**](https://arxiv.org/abs/2403.04253) _Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, Sarath Chandar._ Arxiv 2024.

9.[**Memory Consolidation Enables Long-Context Video Understanding**](https://arxiv.org/abs/2402.05861) _Ivana Balazevic, Yuki M. Asano, Andreas Kirsch, Andrew Zisserman._ Arxiv 2024.

10.[**Online Adaptation of Language Models with a Memory of Amortized Contexts**](https://arxiv.org/abs/2403.04317) _Yongqiang Tack, Seonghyeon Ye, Sangwoo Mo, Jung-Woo Ha, Sung Ju Hwang._ Arxiv 2024.

11.[**InfiniPot: Infinite Context Processing on Memory-Constrained LLMs**](https://arxiv.org/abs/2410.01518) _Jungwook Choi, Yongqiang Tack, Seonghyeon Ye, Sangwoo Mo, Jung-Woo Ha, Sung Ju Hwang._ Arxiv 2024.

12.[**B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory**](https://arxiv.org/abs/2407.06324) _Luca Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, Alessandro Achille, Stefano Soatto._ Arxiv 2024.

13.[**Needle in the Haystack for Memory Based Large Language Models**](https://arxiv.org/abs/2407.01437) _Elliot Nelson, Georgios Kollias, Payel Das, Subhajit Chaudhury, Soham Dan._ Arxiv 2024.

14.[**Memorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk**](https://aclanthology.org/2024.emnlp-main.1169/) _Yunfan Shao, Yunlong Liang, Yongqiang Tack, Seonghyeon Ye, Sangwoo Mo, Jung-Woo Ha, Sung Ju Hwang._ Arxiv 2024.

15.[**Memory Layers at Scale**](https://arxiv.org/abs/2412.09764) _Yongqiang Tack, Seonghyeon Ye, Sangwoo Mo, Jung-Woo Ha, Sung Ju Hwang._ Arxiv 2024.

16.[**Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models**](https://arxiv.org/abs/2309.05605) _Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, Andr√© Bauer, Kyle Chard, Ian Foster._ Arxiv 2023.

17.[**When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models**](https://arxiv.org/abs/2502.15443) _Wenhao Wang, Yunhao Mao, Dingcheng Tang, Haoyu Du, Ning Guan, Yanzhi Xue._ Arxiv 2025.

18.[**MELODI: Exploring Memory Compression for Long Contexts**](https://arxiv.org/abs/2410.03156) _Yinpeng Chen, DeLesley Hutchins, Aren Jansen, Andrey Zhmoginov, David Racz, Jesper Andersen._ Arxiv 2024.

19.[**Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation**](https://arxiv.org/abs/2305.03796) _Chi Fan, Jiaqi Mu, Yiming Yang._ Arxiv 2023.

20.[**Mini-Sequence Transformers: Optimizing Intermediate Memory for Long Sequences Training**](https://arxiv.org/abs/2407.15892) _Yongqiang Tack, Seonghyeon Ye, Sangwoo Mo, Jung-Woo Ha, Sung Ju Hwang._ Arxiv 2024.

21.[**Learn To Remember: Transformer with Recurrent Memory for Document-Level Machine Translation**](https://arxiv.org/abs/2205.01546) _Yukun Feng, Feng Li, Ziang Song, Boyuan Zheng, Philipp Koehn._ Arxiv 2022.

22.[**Linearizing Transformer with Key-Value Memory**](https://arxiv.org/abs/2203.12644) _Yizhe Zhang, Deng Cai._ Arxiv 2022.

23.[**A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information**](https://arxiv.org/abs/2305.07565) _Vladimir Araujo, Andres Soto, Marie-Francine Moens._ Arxiv 2023.

24.[**LaMemo: Language Modeling with Look-Ahead Memory**](https://arxiv.org/abs/2204.07341) _Haoze Ji, Rongcheng Zhang, Zheyang Yang, Zheng Hu, Minlie Huang._ Arxiv 2022.

25.[**An Evolved Universal Transformer Memory**](https://arxiv.org/abs/2410.13166) _Edoardo Cetin, Qi Sun, Tianyu Zhao, Yujin Tang._ Arxiv 2024.

26.[**When Context Leads but Parametric Memory Follows in Large Language Models**](https://arxiv.org/abs/2409.08435) _Yufei Tao, Yujie Qian, Yiming Cui, Wanxiang Che, Ting Liu._ Arxiv 2024.


#### 2.3 Parametric Memory
- 
1.[**Mass-Editing Memory in a Transformer**](https://arxiv.org/abs/2210.07229) _Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau._ Arxiv 2022.

2.[**Memory-Based Model Editing at Scale**](https://arxiv.org/abs/2206.06520) _Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, Chelsea Finn._ Arxiv 2022.

3.[**Memory-assisted prompt editing to improve GPT-3 after deployment**](https://arxiv.org/abs/2201.06009) _Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang._ Arxiv 2022.

4.[**Training Language Models with Memory Augmentation**](https://arxiv.org/abs/2205.12674) _Zexuan Zhong, Tao Lei, Danqi Chen._ Arxiv 2022.

5.[**Efficient Dialogue State Tracking by Selectively Overwriting Memory**](https://arxiv.org/abs/1911.03906) _Sungdong Kim, Sohee Yang, Gyuwan Kim, Donghyun Kwak, Jonghyuck Park, Jong C. Park._ Arxiv 2019.

6.[**Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge**](https://arxiv.org/abs/2502.02173) _Daniel Tamayo, Aitor Gonzalez-Agirre, Javier Gonzalez, Ander Barrena, Montse Cuadros, German Rigau._ Arxiv 2025.

7.[**DSI++: Updating Transformer Memory with New Documents**](https://arxiv.org/abs/2212.09744) _Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Jinfeng Rao, Marc Najork, Emma Strubell, Donald Metzler._ Arxiv 2022.

8.[**Memory3: Language Modeling with Explicit Memory**](https://arxiv.org/abs/2407.01178) _Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Junbo Wang, Zeyun Tang, Shibao Song, Chengyang Xu, Yu Yu, Kui Chen, Feiyu Kong, Longpeng Tang, Wenhao Liu._ Arxiv 2024.

9.[**Information-theoretic Online Memory Selection for Continual Learning**](https://arxiv.org/abs/2204.04763) _Shengyang Sun, Daniele Calandriello, Huiyi Hu, Ang Li, Michalis Titsias._ Arxiv 2022.

10.[**Improving Task-free Continual Learning by Distributionally Robust Memory Evolution**](https://arxiv.org/abs/2207.07256) _Zhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Tiehang Duan, Mingchen Gao._ Arxiv 2022.

11.[**A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm**](https://arxiv.org/abs/2310.12244) _Haizhou Shi, Hao Wang._ Arxiv 2023.

12.[**Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models**](https://arxiv.org/abs/2410.05162) _Yuxiang Wu, Zihan Wang, Yiming Cui, Wanxiang Che, Ting Liu._ Arxiv 2024.

13.[**Sparse Distributed Memory is a Continual Learner**](https://arxiv.org/abs/2303.11934) _Brenton Wiernik, Xander Davies, Deepak Singh, Dmitri Krotov, Gabriel Kreiman._ Arxiv 2023.

14.[**Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation**](https://arxiv.org/abs/2203.11670) _Yingxiu Zhao, Zhiliang Tian, Huaxiu Yao, Yinhe Zheng, Dongkyu Lee, Yiping Song, Jian Sun, Nevin L. Zhang._ Arxiv 2022.

15.[**Content Addressable Memory Without Catastrophic Forgetting by Heteroassociation with a Fixed Scaffold**](https://arxiv.org/abs/2202.00159) _Sugandha Sharma, Sarthak Chandra, Ila R. Fiete._ Arxiv 2022.

16.[**Leitner-Guided Memory Replay for Cross-lingual Continual Learning**](https://aclanthology.org/2024.naacl-long.432/) _Meryem M'hamdi, Jonathan May._ Arxiv 2024.

17.[**Navigating Memory Construction by Global Pseudo-Task Simulation for Continual Learning**](https://arxiv.org/abs/2210.08442) _Yejia Liu, Wang Zhu, Shaolei Ren._ Arxiv 2022.

18.[**Transformer as a Hippocampal Memory Consolidation Model Based on NMDAR-Inspired Nonlinearity**](https://arxiv.org/abs/2504.10886) _Jea Kwon, Jaehoon Lee, Donggyu Kim, Yongmin Park, Jongmin Choi._ Arxiv 2025.

19.[**Mechanisms of Memory Updating: State Dependency vs. Reconsolidation**](https://journalofcognition.org/articles/10.5334/joc.198) _Christopher Kiley, Colleen M. Parks._ Arxiv 2022.

20.[**Think Before You Act: Decision Transformers with Working Memory**](https://arxiv.org/abs/2305.16338) _Jikun Kang, Romain Laroche, Xingdi Yuan, Adam Trischler, Xue Liu, Jie Fu._ Arxiv 2023.

21.[**Improving Factuality with Explicit Working Memory**](https://arxiv.org/abs/2412.18069) _Mingda Chen, Yang Li, Karthik Padthe, Rulin Shao, Alicia Sun, Luke Zettlemoyer, Gargi Ghosh, Wen-tau Yih._ Arxiv 2024.

22. [**AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models**](https://arxiv.org/abs/2410.02355) _Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, Tat-seng Chua_ Arxiv 2025.

23.[**Locating and Editing Factual Associations in GPT**](https://arxiv.org/abs/2202.05262) _Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov_ Arxiv 2023.

24.[**WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models**](https://arxiv.org/abs/2405.14768) _Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen_ Arxiv 2024.

25.[**Can We Edit Factual Knowledge by In-Context Learning?**](https://arxiv.org/abs/2305.12740) _Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, Baobao Chang_ Arxiv 2023.

26.[**Memory Replay with Data Compression for Continual Learning**](https://arxiv.org/abs/2202.06592) _Liyuan Wang, Xingxing Zhang, Kuo Yang, Longhui Yu, Chongxuan Li, Lanqing Hong, Shifeng Zhang, Zhenguo Li, Yi Zhong, Jun Zhu_ Arxiv 2022.

27.[**MEMORYLLM: Towards Self-Updatable Large Language Models**](https://arxiv.org/abs/2402.04624) _Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, Julian McAuley_ Arxiv 2024.


#### 2.4 Multi-source Memory
- 
1.[**ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory**](https://arxiv.org/abs/2306.03901) _Chenxu Hu, Jie Yu, Chencheng Dong, Junbo Zhao, Hang Zhao._ Arxiv 2023.

2.[**DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory**](https://arxiv.org/abs/2410.08143) _Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, Fandong Meng, Jie Zhou, Min Zhang._ Arxiv 2024.

3.[**An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks**](https://arxiv.org/abs/2210.16773) _Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel._ Arxiv 2022.

4.[**Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models**](https://arxiv.org/abs/2310.15127) _Gabriel Sarch, Yuxiang Wu, Yuqi Xie, Yunfan Jiang, Linxi Fan, Ajay M. Patel, Yuke Zhu, Anima Anandkumar._ Arxiv 2023.

5.[**Symbolic Working Memory Enhances Language Models for Complex Rule Application**](https://arxiv.org/abs/2408.13654) _Yuxiang Wang, Jingwei Wei, Yicheng Wang, Zheng Zhang, Yuan-Fang Li._ Arxiv 2024.

6.[**There Are a Thousand Hamlets in a Thousand People's Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory**](https://arxiv.org/abs/2204.02624) _Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, Rui Yan._ Arxiv 2022.

7.[**Prior Knowledge and Memory Enriched Transformer for Sign Language Translation**](https://aclanthology.org/2022.findings-acl.297/) _Tao Jin, Zhou Zhao, Meng Zhang, Xingshan Zeng._ Arxiv 2022.

8.[**G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks**](https://arxiv.org/abs/2212.03613) _Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel._ Arxiv 2022.

9.[**Memory Augmented Language Models through Mixture of Word Experts**](https://arxiv.org/abs/2311.10768) _Cicero Nogueira dos Santos, James Lee-Thorp, Isaac Noble, Chung-Ching Chang, David Uthus._ Arxiv 2023.

10.[**A Cooperative Memory Network for Personalized Task-oriented Dialogue Systems with Incomplete User Profiles**](https://arxiv.org/abs/2102.08322) _Jiahuan Pei, Pengjie Ren, Maarten de Rijke._ Arxiv 2021.

11.[**Memory-aligned Knowledge Graph for Clinically Accurate Radiology Image Report Generation**](https://aclanthology.org/2022.bionlp-1.11/) _Sixing Yan._ Arxiv 2022.

12.[**MATTER: Memory-Augmented Transformer Using Heterogeneous Knowledge Sources**](https://arxiv.org/abs/2406.04670) _Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel._ Arxiv 2024.

13.[**A Framework for Inference Inspired by Human Memory Mechanisms**](https://arxiv.org/abs/2310.09297) _Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel._ Arxiv 2023.

14.[**M3: 3D-Spatial MultiModal Memory**](https://arxiv.org/abs/2503.16413) _Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel._ Arxiv 2025.

15.[**Learning Multimodal Contrast with Cross-modal Memory and Reinforced Contrast Recognition**](https://arxiv.org/abs/2404.00925) _Yuanhe Tian, Fei Xia, Yan Song._ Arxiv 2024.

16.[**Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning**](https://arxiv.org/abs/2401.08523) _Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel._ Arxiv 2024.

## üìä Datasets
- Introduction, usage, and access methods of related datasets.

## üß† Methods
- Overview and comparison of methods used in AI memory research.

## ‚öôÔ∏è Tools
- Tools and libraries for memory modeling and related experiments.

## üåû Future Directions

## Acknowledgements

Please contact me if I miss your names in the list, I will add you back ASAP!

### Contributors

<a href="https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=Elvin-Yiming-Du/Survey_Memory_in_AI" alt="Contributors"/>
</a>

### Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Elvin-Yiming-Du/Survey_Memory_in_AI&type=Timeline)](https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI/stargazers)


