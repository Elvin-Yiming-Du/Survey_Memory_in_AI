
@misc{zhang_survey_2024,
	title = {A {Survey} on the {Memory} {Mechanism} of {Large} {Language} {Model} based {Agents}},
	url = {http://arxiv.org/abs/2404.13501},
	doi = {10.48550/arXiv.2404.13501},
	abstract = {Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at {\textbackslash}url\{https://github.com/nuster1128/LLM\_Agent\_Memory\_Survey\}.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Zhang, Zeyu and Bo, Xiaohe and Ma, Chen and Li, Rui and Chen, Xu and Dai, Quanyu and Zhu, Jieming and Dong, Zhenhua and Wen, Ji-Rong},
	month = apr,
	year = {2024},
	note = {arXiv:2404.13501 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/RIDJ5N4Y/Zhang et al. - 2024 - A Survey on the Memory Mechanism of Large Language Model based Agents.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/7C54YB8A/2404.html:text/html},
}

@misc{huang_advancing_2024,
	title = {Advancing {Transformer} {Architecture} in {Long}-{Context} {Large} {Language} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Advancing {Transformer} {Architecture} in {Long}-{Context} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.12351},
	doi = {10.48550/arXiv.2311.12351},
	abstract = {Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at https://github.com/Strivin0311/long-llms-learning.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Huang, Yunpeng and Xu, Jingwei and Lai, Junyu and Jiang, Zixu and Chen, Taolue and Li, Zenan and Yao, Yuan and Ma, Xiaoxing and Yang, Lijuan and Chen, Hao and Li, Shupeng and Zhao, Penghao},
	month = feb,
	year = {2024},
	note = {arXiv:2311.12351 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/PCNKMV3E/Huang et al. - 2024 - Advancing Transformer Architecture in Long-Context Large Language Models A Comprehensive Survey.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/ALHYXCGJ/2311.html:text/html},
}

@misc{wang_knowledge_2024,
	title = {Knowledge {Editing} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Knowledge {Editing} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.16218},
	doi = {10.48550/arXiv.2310.16218},
	abstract = {Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and Li, Jundong},
	month = sep,
	year = {2024},
	note = {arXiv:2310.16218 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/8FFNNHST/Wang et al. - 2024 - Knowledge Editing for Large Language Models A Survey.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/55I6Z82N/2310.html:text/html},
}

@inproceedings{xu_knowledge_2024,
	address = {Miami, Florida, USA},
	title = {Knowledge {Conflicts} for {LLMs}: {A} {Survey}},
	url = {https://aclanthology.org/2024.emnlp-main.486/},
	doi = {10.18653/v1/2024.emnlp-main.486},
	abstract = {This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Rongwu and Qi, Zehan and Guo, Zhijiang and Wang, Cunxiang and Wang, Hongru and Zhang, Yue and Xu, Wei},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {8541--8565},
	file = {PDF:/Users/savasp/Zotero/storage/65XI8WU3/Xu et al. - 2024 - Knowledge Conflicts for LLMs A Survey.pdf:application/pdf},
}

@inproceedings{li_prompt_2025,
	address = {Albuquerque, New Mexico},
	title = {Prompt {Compression} for {Large} {Language} {Models}: {A} {Survey}},
	isbn = {979-8-89176-189-6},
	url = {https://aclanthology.org/2025.naacl-long.368/},
	abstract = {Leveraging large language models (LLMs) for complex natural language tasks typically requires long-form prompts to convey detailed requirements and information, which results in increased memory usage and inference costs. To mitigate these challenges, multiple efficient methods have been proposed, with prompt compression gaining significant research interest. This survey provides an overview of prompt compression techniques, categorized into hard prompt methods and soft prompt methods. First, the technical approaches of these methods are compared, followed by an exploration of various ways to understand their mechanisms, including the perspectives of attention optimization, Parameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic language. We also examine the downstream adaptations of various prompt compression techniques. Finally, the limitations of current prompt compression methods are analyzed, and several future directions are outlined, such as optimizing the compression encoder, combining hard and soft prompts methods, and leveraging insights from multimodality.},
	booktitle = {Proceedings of the 2025 {Conference} of the {Nations} of the {Americas} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Zongqian and Liu, Yinhong and Su, Yixuan and Collier, Nigel},
	editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
	month = apr,
	year = {2025},
	pages = {7182--7195},
	file = {PDF:/Users/savasp/Zotero/storage/H5LPJWS5/Li et al. - 2025 - Prompt Compression for Large Language Models A Survey.pdf:application/pdf},
}

@misc{geng_comprehensive_2025,
	title = {A {Comprehensive} {Survey} of {Machine} {Unlearning} {Techniques} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2503.01854},
	doi = {10.48550/arXiv.2503.01854},
	abstract = {This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as {\textbackslash}textit\{LLM unlearning\}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Geng, Jiahui and Li, Qing and Woisetschlaeger, Herbert and Chen, Zongxiong and Wang, Yuxia and Nakov, Preslav and Jacobsen, Hans-Arno and Karray, Fakhri},
	month = feb,
	year = {2025},
	note = {arXiv:2503.01854 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/NQYSCPWM/Geng et al. - 2025 - A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/CTAQAET3/2503.html:text/html},
}

@misc{liu_survey_2025,
	title = {A {Survey} of {Personalized} {Large} {Language} {Models}: {Progress} and {Future} {Directions}},
	shorttitle = {A {Survey} of {Personalized} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2502.11528},
	doi = {10.48550/arXiv.2502.11528},
	abstract = {Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Liu, Jiahong and Qiu, Zexuan and Li, Zhongyang and Dai, Quanyu and Zhu, Jieming and Hu, Minda and Yang, Menglin and King, Irwin},
	month = feb,
	year = {2025},
	note = {arXiv:2502.11528 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/5T8RDNED/Liu et al. - 2025 - A Survey of Personalized Large Language Models Progress and Future Directions.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/MTBLHWF4/2502.html:text/html},
}

@misc{he_human-inspired_2025,
	title = {Human-inspired {Perspectives}: {A} {Survey} on {AI} {Long}-term {Memory}},
	shorttitle = {Human-inspired {Perspectives}},
	url = {http://arxiv.org/abs/2411.00489},
	doi = {10.48550/arXiv.2411.00489},
	abstract = {With the rapid advancement of AI systems, their abilities to store, retrieve, and utilize information over the long term - referred to as long-term memory - have become increasingly significant. These capabilities are crucial for enhancing the performance of AI systems across a wide range of tasks. However, there is currently no comprehensive survey that systematically investigates AI's long-term memory capabilities, formulates a theoretical framework, and inspires the development of next-generation AI long-term memory systems. This paper begins by introducing the mechanisms of human long-term memory, then explores AI long-term memory mechanisms, establishing a mapping between the two. Based on the mapping relationships identified, we extend the current cognitive architectures and propose the Cognitive Architecture of Self-Adaptive Long-term Memory (SALM). SALM provides a theoretical framework for the practice of AI long-term memory and holds potential for guiding the creation of next-generation long-term memory driven AI systems. Finally, we delve into the future directions and application prospects of AI long-term memory.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {He, Zihong and Lin, Weizhe and Zheng, Hao and Zhang, Fan and Jones, Matt W. and Aitchison, Laurence and Xu, Xuhai and Liu, Miao and Kristensson, Per Ola and Shen, Junxiao},
	month = jan,
	year = {2025},
	note = {arXiv:2411.00489 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/IPHYN9XA/He et al. - 2025 - Human-inspired Perspectives A Survey on AI Long-term Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/SLNKWRFW/2411.html:text/html},
}

@misc{shan_cognitive_2025,
	title = {Cognitive {Memory} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2504.02441},
	doi = {10.48550/arXiv.2504.02441},
	abstract = {This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Shan, Lianlei and Luo, Shixian and Zhu, Zezhou and Yuan, Yu and Wu, Yong},
	month = apr,
	year = {2025},
	note = {arXiv:2504.02441 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/PTFZHXR5/Shan et al. - 2025 - Cognitive Memory in Large Language Models.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/B23GYRTR/2504.html:text/html},
}

@misc{liu_comprehensive_2025,
	title = {A {Comprehensive} {Survey} on {Long} {Context} {Language} {Modeling}},
	url = {http://arxiv.org/abs/2503.17407},
	doi = {10.48550/arXiv.2503.17407},
	abstract = {Efficient processing of long contexts has been a persistent pursuit in Natural Language Processing. With the growing number of long documents, dialogues, and other textual data, it is important to develop Long Context Language Models (LCLMs) that can process and analyze extensive inputs in an effective and efficient way. In this paper, we present a comprehensive survey on recent advances in long-context modeling for large language models. Our survey is structured around three key aspects: how to obtain effective and efficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate and analyze LCLMs comprehensively. For the first aspect, we discuss data strategies, architectural designs, and workflow approaches oriented with long context processing. For the second aspect, we provide a detailed examination of the infrastructure required for LCLM training and inference. For the third aspect, we present evaluation paradigms for long-context comprehension and long-form generation, as well as behavioral analysis and mechanism interpretability of LCLMs. Beyond these three key aspects, we thoroughly explore the diverse application scenarios where existing LCLMs have been deployed and outline promising future development directions. This survey provides an up-to-date review of the literature on long-context LLMs, which we wish to serve as a valuable resource for both researchers and engineers. An associated GitHub repository collecting the latest papers and repos is available at: {\textbackslash}href\{https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling\}\{{\textbackslash}color[RGB]\{175,36,67\}\{LCLM-Horizon\}\}.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Liu, Jiaheng and Zhu, Dawei and Bai, Zhiqi and He, Yancheng and Liao, Huanxuan and Que, Haoran and Wang, Zekun and Zhang, Chenchen and Zhang, Ge and Zhang, Jiebin and Zhang, Yuanxing and Chen, Zhuo and Guo, Hangyu and Li, Shilong and Liu, Ziqiang and Shan, Yong and Song, Yifan and Tian, Jiayi and Wu, Wenhao and Zhou, Zhejian and Zhu, Ruijie and Feng, Junlan and Gao, Yang and He, Shizhu and Li, Zhoujun and Liu, Tianyu and Meng, Fanyu and Su, Wenbo and Tan, Yingshui and Wang, Zili and Yang, Jian and Ye, Wei and Zheng, Bo and Zhou, Wangchunshu and Huang, Wenhao and Li, Sujian and Zhang, Zhaoxiang},
	month = mar,
	year = {2025},
	note = {arXiv:2503.17407 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/B4TUDKP7/Liu et al. - 2025 - A Comprehensive Survey on Long Context Language Modeling.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/QVIIVAID/2503.html:text/html},
}

@inproceedings{chen_compress_2025,
	address = {Abu Dhabi, UAE},
	title = {Compress to {Impress}: {Unleashing} the {Potential} of {Compressive} {Memory} in {Real}-{World} {Long}-{Term} {Conversations}},
	url = {https://aclanthology.org/2025.coling-main.51/},
	abstract = {Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a “One-for-All” approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which integrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we collect the biggest Chinese long-term conversation dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY`s superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Nuo and Li, Hongguang and Chang, Jianhui and Huang, Juhua and Wang, Baoyuan and Li, Jia},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
	month = jan,
	year = {2025},
	pages = {755--773},
	file = {PDF:/Users/savasp/Zotero/storage/KUBD6QLF/Chen et al. - 2025 - Compress to Impress Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversa.pdf:application/pdf},
}
