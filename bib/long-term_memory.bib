
@article{zhong_memorybank_2024,
	title = {{MemoryBank}: {Enhancing} {Large} {Language} {Models} with {Long}-{Term} {Memory}},
	volume = {38},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{MemoryBank}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29946},
	doi = {10.1609/aaai.v38i17.29946},
	abstract = {Large Language Models (LLMs) have drastically reshaped our interactions with artificial intelligence (AI) systems, showcasing impressive performance across an extensive array of tasks. Despite this, a notable hindrance remains—the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems, psychological counseling, and secretarial assistance. Recognizing the necessity for long-term memory, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user's personality over time by synthesizing information from previous interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory. This mechanism permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a more human-like memory mechanism and enriched user experience. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models such as ChatGLM. To validate MemoryBank's effectiveness, we exemplify its application through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialog data, SiliconFriend displays heightened empathy and discernment in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as multiple users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.},
	number = {17},
	urldate = {2025-05-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Ye, He and Wang, Yanlin},
	month = mar,
	year = {2024},
	pages = {19724--19731},
	file = {Full Text:/Users/savasp/Zotero/storage/HF8SL7G4/Zhong et al. - 2024 - MemoryBank Enhancing Large Language Models with Long-Term Memory.pdf:application/pdf},
}

@inproceedings{maharana_evaluating_2024,
	address = {Bangkok, Thailand},
	title = {Evaluating {Very} {Long}-{Term} {Conversational} {Memory} of {LLM} {Agents}},
	url = {https://aclanthology.org/2024.acl-long.747/},
	doi = {10.18653/v1/2024.acl-long.747},
	abstract = {Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 600 turns and 16K tokens on avg., over up to 32 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Maharana, Adyasha and Lee, Dong-Ho and Tulyakov, Sergey and Bansal, Mohit and Barbieri, Francesco and Fang, Yuwei},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {13851--13870},
	file = {PDF:/Users/savasp/Zotero/storage/QAG4QBJG/Maharana et al. - 2024 - Evaluating Very Long-Term Conversational Memory of LLM Agents.pdf:application/pdf},
}

@misc{xu_-mem_2025,
	title = {A-{MEM}: {Agentic} {Memory} for {LLM} {Agents}},
	shorttitle = {A-{MEM}},
	url = {http://arxiv.org/abs/2502.12110},
	doi = {10.48550/arXiv.2502.12110},
	abstract = {While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/AgenticMemory, while the source code of agentic memory system is available at https://github.com/agiresearch/A-mem.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Xu, Wujiang and Mei, Kai and Gao, Hang and Tan, Juntao and Liang, Zujie and Zhang, Yongfeng},
	month = apr,
	year = {2025},
	note = {arXiv:2502.12110 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/BBVXIBZM/Xu et al. - 2025 - A-MEM Agentic Memory for LLM Agents.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/MMH369M5/2502.html:text/html},
}

@inproceedings{xu_beyond_2022,
	address = {Dublin, Ireland},
	title = {Beyond {Goldfish} {Memory}: {Long}-{Term} {Open}-{Domain} {Conversation}},
	url = {https://aclanthology.org/2022.acl-long.356/},
	doi = {10.18653/v1/2022.acl-long.356},
	abstract = {Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other`s interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Jing and Szlam, Arthur and Weston, Jason},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {5180--5197},
	file = {PDF:/Users/savasp/Zotero/storage/K5GLYRLP/Xu et al. - 2022 - Beyond Goldfish Memory Long-Term Open-Domain Conversation.pdf:application/pdf},
}

@inproceedings{li_optimus-1_2024,
	title = {Optimus-1: {Hybrid} {Multimodal} {Memory} {Empowered} {Agents} {Excel} in {Long}-{Horizon} {Tasks}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5949a8750a110ce1f0631b1776c500a2-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Zaijing and Xie, Yuquan and Shao, Rui and Chen, Gongwei and Jiang, Dongmei and Nie, Liqiang},
	editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
	year = {2024},
	pages = {49881--49913},
	file = {NeurIPS-2024-optimus-1-hybrid-multimodal-memory-empowered-agents-excel-in-long-horizon-tasks-Paper-Conference:/Users/savasp/tmp/NeurIPS-2024-optimus-1-hybrid-multimodal-memory-empowered-agents-excel-in-long-horizon-tasks-Paper-Conference.pdf:application/pdf},
}

@inproceedings{gutierrez_hipporag_2024,
	title = {{HippoRAG}: {Neurobiologically} {Inspired} {Long}-{Term} {Memory} for {Large} {Language} {Models}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/6ddc001d07ca4f319af96a3024f6dbd1-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gutiérrez, Bernal Jiménez and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
	editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
	year = {2024},
	pages = {59532--59569},
	file = {PDF:/Users/savasp/Zotero/storage/9AZTPP2N/Gutiérrez et al. - 2024 - HippoRAG Neurobiologically Inspired Long-Term Memory for Large Language Models.pdf:application/pdf},
}

@inproceedings{xu_long_2022,
	address = {Dublin, Ireland},
	title = {Long {Time} {No} {See}! {Open}-{Domain} {Conversation} with {Long}-{Term} {Persona} {Memory}},
	url = {https://aclanthology.org/2022.findings-acl.207/},
	doi = {10.18653/v1/2022.findings-acl.207},
	abstract = {Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework with Long-Term Memory (LTM) mechanism (called PLATO-LTM). This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Xinchao and Gou, Zhibin and Wu, Wenquan and Niu, Zheng-Yu and Wu, Hua and Wang, Haifeng and Wang, Shihang},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {2639--2650},
	file = {PDF:/Users/savasp/Zotero/storage/BB6GY5FB/Xu et al. - 2022 - Long Time No See! Open-Domain Conversation with Long-Term Persona Memory.pdf:application/pdf},
}

@misc{wu_longmemeval_2025,
	title = {{LongMemEval}: {Benchmarking} {Chat} {Assistants} on {Long}-{Term} {Interactive} {Memory}},
	shorttitle = {{LongMemEval}},
	url = {http://arxiv.org/abs/2410.10813},
	doi = {10.48550/arXiv.2410.10813},
	abstract = {Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. We introduce LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing a 30\% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into three stages: indexing, retrieval, and reading. Built upon key experimental insights, we propose several memory design optimizations including session decomposition for value granularity, fact-augmented key expansion for indexing, and time-aware query expansion for refining the search scope. Extensive experiments show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI. Our benchmark and code are publicly available at https://github.com/xiaowu0162/LongMemEval.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Wu, Di and Wang, Hongwei and Yu, Wenhao and Zhang, Yuwei and Chang, Kai-Wei and Yu, Dong},
	month = mar,
	year = {2025},
	note = {arXiv:2410.10813 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/98FJGWFL/Wu et al. - 2025 - LongMemEval Benchmarking Chat Assistants on Long-Term Interactive Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/55TTL7I9/2410.html:text/html},
}

@inproceedings{hou_my_2024,
	title = {"{My} agent understands me better": {Integrating} {Dynamic} {Human}-like {Memory} {Recall} and {Consolidation} in {LLM}-{Based} {Agents}},
	shorttitle = {"{My} agent understands me better"},
	url = {http://arxiv.org/abs/2404.00573},
	doi = {10.1145/3613905.3650839},
	abstract = {In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of large language model based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of LLMs. We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user's interaction history in a database that encapsulates each memory's content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences.},
	urldate = {2025-05-12},
	booktitle = {Extended {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Hou, Yuki and Tamoto, Haruki and Miyashita, Homei},
	month = may,
	year = {2024},
	note = {arXiv:2404.00573 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	pages = {1--7},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/AQY54VJR/Hou et al. - 2024 - My agent understands me better Integrating Dynamic Human-like Memory Recall and Consolidation in.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/TJPKPFTG/2404.html:text/html},
}

@inproceedings{bae_keep_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Keep {Me} {Updated}! {Memory} {Management} in {Long}-term {Conversations}},
	url = {https://aclanthology.org/2022.findings-emnlp.276/},
	doi = {10.18653/v1/2022.findings-emnlp.276},
	abstract = {Remembering important information from the past and continuing to talk about it in the present are crucial in long-term conversations. However, previous literature does not deal with cases where the memorized information is outdated, which may cause confusion in later conversations. To address this issue, we present a novel task and a corresponding dataset of memory management in long-term conversations, in which bots keep track of and bring up the latest information about users while conversing through multiple sessions. In order to support more precise and interpretable memory, we represent memory as unstructured text descriptions of key information and propose a new mechanism of memory management that selectively eliminates invalidated or redundant information. Experimental results show that our approach outperforms the baselines that leave the stored memory unchanged in terms of engagingness and humanness, with larger performance gap especially in the later sessions.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Bae, Sanghwan and Kwak, Donghyun and Kang, Soyoung and Lee, Min Young and Kim, Sungdong and Jeong, Yuin and Kim, Hyeri and Lee, Sang-Woo and Park, Woomyoung and Sung, Nako},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {3769--3787},
	file = {PDF:/Users/savasp/Zotero/storage/VQICCPYH/Bae et al. - 2022 - Keep Me Updated! Memory Management in Long-term Conversations.pdf:application/pdf},
}

@inproceedings{li_mot_2023,
	address = {Singapore},
	title = {{MoT}: {Memory}-of-{Thought} {Enables} {ChatGPT} to {Self}-{Improve}},
	url = {https://aclanthology.org/2023.emnlp-main.392/},
	doi = {10.18653/v1/2023.emnlp-main.392},
	abstract = {Large Language Models (LLMs) have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, **MoT**, to let the LLM self-improve through **M**emory **o**f **T**houghts, without annotated datasets and parameter updates. Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2. During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Xiaonan and Qiu, Xipeng},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {6354--6374},
	file = {PDF:/Users/savasp/Zotero/storage/VHFIM32X/Li and Qiu - 2023 - MoT Memory-of-Thought Enables ChatGPT to Self-Improve.pdf:application/pdf},
}

@misc{wang_recursively_2025,
	title = {Recursively {Summarizing} {Enables} {Long}-{Term} {Dialogue} {Memory} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.15022},
	doi = {10.48550/arXiv.2308.15022},
	abstract = {Recently, large language models (LLMs), such as GPT-4, stand out remarkable conversational abilities, enabling them to engage in dynamic and contextually relevant dialogues across a wide range of topics. However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses. To address this, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the chatbot can easily generate a highly consistent response with the help of the latest memory. We evaluate our method on both open and closed LLMs, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Also, we show that our strategy could nicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced LLMs, bringing further long-term dialogue performance. Notably, our method is a potential solution to enable the LLM to model the extremely long context. The code and scripts will be released later.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Wang, Qingyue and Fu, Yanhe and Cao, Yanan and Wang, Shuai and Tian, Zhiliang and Ding, Liang},
	month = may,
	year = {2025},
	note = {arXiv:2308.15022 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/TNWR9JEA/Wang et al. - 2025 - Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/DPJDIIY7/2308.html:text/html},
}

@misc{park_mrsteve_2025,
	title = {{MrSteve}: {Instruction}-{Following} {Agents} in {Minecraft} with {What}-{Where}-{When} {Memory}},
	shorttitle = {{MrSteve}},
	url = {http://arxiv.org/abs/2411.06736},
	doi = {10.48550/arXiv.2411.06736},
	abstract = {Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce MrSteve (Memory Recall Steve), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: https://sites.google.com/view/mr-steve.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Park, Junyeong and Cho, Junmo and Ahn, Sungjin},
	month = apr,
	year = {2025},
	note = {arXiv:2411.06736 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/DTHBV9KI/Park et al. - 2025 - MrSteve Instruction-Following Agents in Minecraft with What-Where-When Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/NBWLUKV7/2411.html:text/html},
}

@misc{zhang_llm-based_2024,
	title = {{LLM}-based {Medical} {Assistant} {Personalization} with {Short}- and {Long}-{Term} {Memory} {Coordination}},
	url = {http://arxiv.org/abs/2309.11696},
	doi = {10.48550/arXiv.2309.11696},
	abstract = {Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, medical assistants hold the potential to offer substantial benefits for individuals. However, the exploration of LLM-based personalized medical assistant remains relatively scarce. Typically, patients converse differently based on their background and preferences which necessitates the task of enhancing user-oriented medical assistant. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to enhance the response with aware of previous mistakes for new queries during a dialogue session. We contend that a mere memory module is inadequate and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, to personalize medical assistants.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Zhang, Kai and Kang, Yangyang and Zhao, Fubang and Liu, Xiaozhong},
	month = apr,
	year = {2024},
	note = {arXiv:2309.11696 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/3NVJDUAV/Zhang et al. - 2024 - LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/8ISRC6CX/2309.html:text/html},
}

@misc{wang_scm_2025,
	title = {{SCM}: {Enhancing} {Large} {Language} {Model} with {Self}-{Controlled} {Memory} {Framework}},
	shorttitle = {{SCM}},
	url = {http://arxiv.org/abs/2304.13343},
	doi = {10.48550/arXiv.2304.13343},
	abstract = {Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information. To address this limitation, in this paper, we propose the Self-Controlled Memory (SCM) framework to enhance the ability of LLMs to maintain long-term memory and recall relevant information. Our SCM framework comprises three key components: an LLM-based agent serving as the backbone of the framework, a memory stream storing agent memories, and a memory controller updating memories and determining when and how to utilize memories from memory stream. Additionally, the proposed SCM is able to process ultra-long texts without any modification or fine-tuning, which can integrate with any instruction following LLMs in a plug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the effectiveness of SCM for handling lengthy inputs. The annotated dataset covers three tasks: long-term dialogues, book summarization, and meeting summarization. Experimental results demonstrate that our method achieves better retrieval recall and generates more informative responses compared to competitive baselines in long-term dialogues. (https://github.com/wbbeyourself/SCM4LLMs)},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Wang, Bing and Liang, Xinnian and Yang, Jian and Huang, Hui and Wu, Shuangzhi and Wu, Peihao and Lu, Lu and Ma, Zejun and Li, Zhoujun},
	month = mar,
	year = {2025},
	note = {arXiv:2304.13343 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/D7S64T3U/Wang et al. - 2025 - SCM Enhancing Large Language Model with Self-Controlled Memory Framework.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/EH7W9U9S/2304.html:text/html},
}

@inproceedings{dalvi_mishra_towards_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Towards {Teachable} {Reasoning} {Systems}: {Using} a {Dynamic} {Memory} of {User} {Feedback} for {Continual} {System} {Improvement}},
	url = {https://aclanthology.org/2022.emnlp-main.644/},
	doi = {10.18653/v1/2022.emnlp-main.644},
	abstract = {Our goal is a teachable reasoning system for question-answering (QA), where a user can interact with faithful answer explanations, and correct its errors so that the system improves over time. Our approach is to augment a QA model with a dynamic memory of user feedback, containing user-supplied corrections toerroneous model beliefs that users identify during interaction. Retrievals from memory are used as additional context for QA, to help avoid previous mistakes in similar new situations - a novel application of memory-based continuous learning. With simulated feedback, we find that our system (called TeachMe) continually improves with time, and without model retraining, requiring feedback on only 25\% of training examples to reach within 1\% of the upper-bound (feedback on all examples). Similarly, in experiments with real users, we observe a similar trend, with performance improving by over 15\% on a hidden test set after teaching. This suggests new opportunities for using frozen language models in an interactive setting where users can inspect, debug, and correct the model`s beliefs, leading to improved system`s performance over time.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Dalvi Mishra, Bhavana and Tafjord, Oyvind and Clark, Peter},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {9465--9480},
	file = {PDF:/Users/savasp/Zotero/storage/KJML79FC/Dalvi Mishra et al. - 2022 - Towards Teachable Reasoning Systems Using a Dynamic Memory of User Feedback for Continual System Im.pdf:application/pdf},
}

@inproceedings{wang_stablessm_2024,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{StableSSM}: {Alleviating} the {Curse} of {Memory} in {State}-space {Models} through {Stable} {Reparameterization}},
	volume = {235},
	url = {https://proceedings.mlr.press/v235/wang24ag.html},
	abstract = {In this paper, we investigate the long-term memory learning capabilities of state-space models (SSMs) from the perspective of parameterization. We prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory. Our analysis identifies this “curse of memory” as a result of the recurrent weights converging to a stability boundary, suggesting that a reparameterization technique can be effective. To this end, we introduce a class of reparameterization techniques for SSMs that effectively lift its memory limitations. Besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability. We validate our findings using synthetic datasets, language models and image classifications.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Shida and Li, Qianxiao},
	editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
	month = jul,
	year = {2024},
	pages = {50766--50793},
}

@inproceedings{wang_crafting_2024,
	address = {Miami, Florida, USA},
	title = {Crafting {Personalized} {Agents} through {Retrieval}-{Augmented} {Generation} on {Editable} {Memory} {Graphs}},
	url = {https://aclanthology.org/2024.emnlp-main.281},
	doi = {10.18653/v1/2024.emnlp-main.281},
	language = {en},
	urldate = {2025-05-12},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Zheng and Li, Zhongyang and Jiang, Zeren and Tu, Dandan and Shi, Wei},
	year = {2024},
	pages = {4891--4906},
	file = {Submitted Version:/Users/savasp/Zotero/storage/3HLRBWK9/Wang et al. - 2024 - Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs.pdf:application/pdf},
}

@inproceedings{tandon_learning_2022,
	address = {Seattle, United States},
	title = {Learning to repair: {Repairing} model output errors after deployment using a dynamic memory of feedback},
	url = {https://aclanthology.org/2022.findings-naacl.26/},
	doi = {10.18653/v1/2022.findings-naacl.26},
	abstract = {Large language models (LMs), while powerful, are not immune to mistakes, but can be difficult to retrain. Our goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user. Our approach pairs an LM with (i) a growing memory of cases where the user identified an output error and provided general feedback on how to correct it (ii) a \textit{corrector model}, trained to translate this general feedback into specific edits to repair the model output. Given a new, unseen input, our model can then use feedback from similar, past cases to repair output errors that may occur. We instantiate our approach using an existing, fixed model for \textit{script generation}, that takes a goal (e.g., “bake a cake”) and generates a partially ordered sequence of actions to achieve that goal, sometimes containing errors. Our memory-enhanced system, , learns to apply user feedback to repair such errors (up to 30 points improvement), while making a start at avoiding similar past mistakes on new, unseen examples (up to 7 points improvement in a controlled setting). This is a first step towards strengthening deployed models, potentially broadening their utility. Our code and data is available at https://github.com/allenai/interscript},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Tandon, Niket and Madaan, Aman and Clark, Peter and Yang, Yiming},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {339--352},
	file = {PDF:/Users/savasp/Zotero/storage/G6H9TJ5X/Tandon et al. - 2022 - Learning to repair Repairing model output errors after deployment using a dynamic memory of feedbac.pdf:application/pdf},
}

@inproceedings{sun_towards_2024,
	address = {Miami, Florida, USA},
	title = {Towards {Verifiable} {Text} {Generation} with {Evolving} {Memory} and {Self}-{Reflection}},
	url = {https://aclanthology.org/2024.emnlp-main.469/},
	doi = {10.18653/v1/2024.emnlp-main.469},
	abstract = {Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination. A promising solution to this issue is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the intricate reasoning needed to align the claim with correct citations, and the dilemma between the precision and breadth of retrieved documents. In this paper, we present VTG, an innovative framework for Verifiable Text Generation with evolving memory and self-reflection. VTG introduces evolving long short-term memory to retain both valuable documents and recent documents. A two-tier verifier equipped with an evidence finder is proposed to rethink and reflect on the relationship between the claim and citations. Furthermore, active retrieval and diverse query generation are utilized to enhance both the precision and breadth of the retrieved documents. We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms baselines.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Hao and Cai, Hengyi and Wang, Bo and Hou, Yingyan and Wei, Xiaochi and Wang, Shuaiqiang and Zhang, Yan and Yin, Dawei},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {8211--8227},
	file = {PDF:/Users/savasp/Zotero/storage/JCBH9KDQ/Sun et al. - 2024 - Towards Verifiable Text Generation with Evolving Memory and Self-Reflection.pdf:application/pdf},
}

@inproceedings{fu_there_2022,
	address = {Dublin, Ireland},
	title = {There {Are} a {Thousand} {Hamlets} in a {Thousand} {People}`s {Eyes}: {Enhancing} {Knowledge}-grounded {Dialogue} with {Personal} {Memory}},
	url = {https://aclanthology.org/2022.acl-long.270/},
	doi = {10.18653/v1/2022.acl-long.270},
	abstract = {Knowledge-grounded conversation (KGC) shows great potential in building an engaging and knowledgeable chatbot, and knowledge selection is a key ingredient in it. However, previous methods for knowledge selection only concentrate on the relevance between knowledge and dialogue context, ignoring the fact that age, hobby, education and life experience of an interlocutor have a major effect on his or her personal preference over external knowledge. Without taking the personalization issue into account, it is difficult for existing dialogue systems to select the proper knowledge and generate persona-consistent responses. In this work, we introduce personal memory into knowledge selection in KGC to address the personalization issue. We propose a variational method to model the underlying relationship between one`s personal memory and his or her selection of knowledge, and devise a learning scheme in which the forward mapping from personal memory to knowledge and its inverse mapping is included in a closed loop so that they could teach each other. Experiment results show that our methods outperform existing KGC methods significantly on both automatic evaluation and human evaluation.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fu, Tingchen and Zhao, Xueliang and Tao, Chongyang and Wen, Ji-Rong and Yan, Rui},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {3901--3913},
	file = {PDF:/Users/savasp/Zotero/storage/IKSS73F2/Fu et al. - 2022 - There Are a Thousand Hamlets in a Thousand People`s Eyes Enhancing Knowledge-grounded Dialogue with.pdf:application/pdf},
}

@misc{du_perltqa_2024,
	title = {{PerLTQA}: {A} {Personal} {Long}-{Term} {Memory} {Dataset} for {Memory} {Classification}, {Retrieval}, and {Synthesis} in {Question} {Answering}},
	shorttitle = {{PerLTQA}},
	url = {http://arxiv.org/abs/2402.16288},
	doi = {10.48550/arXiv.2402.16288},
	abstract = {Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task. PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs). Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis. We evaluate this framework using five LLMs and three retrievers. Experimental results demonstrate that BERT-based classification models significantly outperform LLMs such as ChatGLM3 and ChatGPT in the memory classification task. Furthermore, our study highlights the importance of effective memory integration in the QA task.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Du, Yiming and Wang, Hongru and Zhao, Zhengyi and Liang, Bin and Wang, Baojun and Zhong, Wanjun and Wang, Zezhong and Wong, Kam-Fai},
	month = feb,
	year = {2024},
	note = {arXiv:2402.16288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/8SU3GS2I/Du et al. - 2024 - PerLTQA A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/KRIGP9TD/2402.html:text/html},
}

@inproceedings{pei_cooperative_2021,
	address = {New York, NY, USA},
	series = {{WWW} '21},
	title = {A {Cooperative} {Memory} {Network} for {Personalized} {Task}-oriented {Dialogue} {Systems} with {Incomplete} {User} {Profiles}},
	isbn = {978-1-4503-8312-7},
	url = {https://doi.org/10.1145/3442381.3449843},
	doi = {10.1145/3442381.3449843},
	abstract = {There is increasing interest in developing personalized Task-oriented Dialogue Systems (TDSs). Previous work on personalized TDSs often assumes that complete user profiles are available for most or even all users. This is unrealistic because In this paper, we study personalized TDSs without assuming that user profiles are complete. We propose a Cooperative Memory Network (CoMemNN) that has a novel mechanism to gradually enrich user profiles as dialogues progress and to simultaneously improve response selection based on the enriched profiles. Cooperative Memory Network (CoMemNN) consists of two core modules: User Profile Enrichment (UPE) and Dialogue Response Selection (DRS). The former enriches incomplete user profiles by utilizing collaborative information from neighbor users as well as current dialogues. The latter uses the enriched profiles to update the current user query so as to encode more useful information, based on which a personalized response to a user request is selected. We conduct extensive experiments on the personalized bAbI dialogue benchmark datasets. We find that CoMemNN is able to enrich user profiles effectively, which results in an improvement of 3.06\% in terms of response selection accuracy compared to state-of-the-art methods. We also test the robustness of CoMemNN against incompleteness of user profiles by randomly discarding attribute values from user profiles. Even when discarding 50\% of the attribute values, CoMemNN is able to match the performance of the best performing baseline without discarding user profiles, showing the robustness of CoMemNN.},
	urldate = {2025-05-12},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Pei, Jiahuan and Ren, Pengjie and de Rijke, Maarten},
	month = jun,
	year = {2021},
	pages = {1552--1561},
	file = {Full Text:/Users/savasp/Zotero/storage/USHX7Z2N/Pei et al. - 2021 - A Cooperative Memory Network for Personalized Task-oriented Dialogue Systems with Incomplete User Pr.pdf:application/pdf},
}

@inproceedings{chen_compress_2025,
	address = {Abu Dhabi, UAE},
	title = {Compress to {Impress}: {Unleashing} the {Potential} of {Compressive} {Memory} in {Real}-{World} {Long}-{Term} {Conversations}},
	url = {https://aclanthology.org/2025.coling-main.51/},
	abstract = {Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a “One-for-All” approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which integrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we collect the biggest Chinese long-term conversation dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY`s superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Nuo and Li, Hongguang and Chang, Jianhui and Huang, Juhua and Wang, Baoyuan and Li, Jia},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
	month = jan,
	year = {2025},
	pages = {755--773},
	file = {PDF:/Users/savasp/Zotero/storage/IF6PCNR3/Chen et al. - 2025 - Compress to Impress Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversa.pdf:application/pdf},
}

@inproceedings{yang_iterative_2024,
	address = {Bangkok, Thailand},
	title = {An {Iterative} {Associative} {Memory} {Model} for {Empathetic} {Response} {Generation}},
	url = {https://aclanthology.org/2024.acl-long.170/},
	doi = {10.18653/v1/2024.acl-long.170},
	abstract = {Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances.We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression.},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Zhou and Ren, Zhaochun and Yufeng, Wang and Sun, Haizhou and Chen, Chao and Zhu, Xiaofei and Liao, Xiangwen},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {3081--3092},
	file = {PDF:/Users/savasp/Zotero/storage/RSTG89HR/Yang et al. - 2024 - An Iterative Associative Memory Model for Empathetic Response Generation.pdf:application/pdf},
}

@inproceedings{ong_towards_2025,
	address = {Albuquerque, New Mexico},
	title = {Towards {Lifelong} {Dialogue} {Agents} via {Timeline}-based {Memory} {Management}},
	isbn = {979-8-89176-189-6},
	url = {https://aclanthology.org/2025.naacl-long.435/},
	abstract = {To achieve lifelong human-agent interaction, dialogue agents need to constantly memorize perceived information and properly retrieve it for response generation (RG). While prior studies focus on getting rid of outdated memories to improve retrieval quality, we argue that such memories provide rich, important contextual cues for RG (e.g., changes in user behaviors) in long-term conversations. We present THEANINE, a framework for LLM-based lifelong dialogue agents. THEANINE discards memory removal and manages large-scale memories by linking them based on their temporal and cause-effect relation. Enabled by this linking structure, THEANINE augments RG with memory timelines - series of memories representing the evolution or causality of relevant past events. Along with THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme, addressing the limitation of G-Eval and human efforts when assessing agent performance in integrating past memories into RG. A supplementary video for THEANINE and data for TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.},
	booktitle = {Proceedings of the 2025 {Conference} of the {Nations} of the {Americas} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ong, Kai Tzu-iunn and Kim, Namyoung and Gwak, Minju and Chae, Hyungjoo and Kwon, Taeyoon and Jo, Yohan and Hwang, Seung-won and Lee, Dongha and Yeo, Jinyoung},
	editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
	month = apr,
	year = {2025},
	pages = {8631--8661},
	file = {PDF:/Users/savasp/Zotero/storage/3PBUATAQ/Ong et al. - 2025 - Towards Lifelong Dialogue Agents via Timeline-based Memory Management.pdf:application/pdf},
}

@misc{lee_cocoa_2024,
	title = {{COCOA}: {CBT}-based {Conversational} {Counseling} {Agent} using {Memory} {Specialized} in {Cognitive} {Distortions} and {Dynamic} {Prompt}},
	shorttitle = {{COCOA}},
	url = {http://arxiv.org/abs/2402.17546},
	doi = {10.48550/arXiv.2402.17546},
	abstract = {The demand for conversational agents that provide mental health care is consistently increasing. In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client's statements. Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances. Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic prompting to flexibly apply CBT techniques and facilitate the appropriate retrieval of information. We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation. Then, we asked GPT to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from other models.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Lee, Suyeon and Kang, Jieun and Kim, Harim and Chung, Kyoung-Mee and Lee, Dongha and Yeo, Jinyoung},
	month = feb,
	year = {2024},
	note = {arXiv:2402.17546 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/ZEUXBQLJ/Lee et al. - 2024 - COCOA CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions a.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/BPI36F2K/2402.html:text/html},
}

@inproceedings{jang_mixed-session_2024,
	address = {Miami, Florida, USA},
	title = {Mixed-{Session} {Conversation} with {Egocentric} {Memory}},
	url = {https://aclanthology.org/2024.findings-emnlp.689/},
	doi = {10.18653/v1/2024.findings-emnlp.689},
	abstract = {Recently introduced dialogue systems have demonstrated high usability. However, they still fall short of reflecting real-world conversation scenarios. Current dialogue systems exhibit an inability to replicate the dynamic, continuous, long-term interactions involving multiple partners. This shortfall arises because there have been limited efforts to account for both aspects of real-world dialogues: deeply layered interactions over the long-term dialogue and widely expanded conversation networks involving multiple participants. As the effort to incorporate these aspects combined, we introduce Mixed-Session Conversation, a dialogue system designed to construct conversations with various partners in a multi-session dialogue setup. We propose a new dataset called MiSC to implement this system. The dialogue episodes of MiSC consist of 6 consecutive sessions, with four speakers (one main speaker and three partners) appearing in each episode. Also, we propose a new dialogue model with a novel memory management mechanism, called Egocentric Memory Enhanced Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories from the main speaker`s perspective during conversations with partners, enabling seamless continuity in subsequent interactions. Extensive human evaluations validate that the dialogues in MiSC demonstrate a seamless conversational flow, even when conversation partners change in each session. EMMA trained with MiSC is also evaluated to maintain high memorability without contradiction throughout the entire conversation.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Jang, Jihyoung and Kim, Taeyoung and Kim, Hyounghun},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {11786--11815},
	file = {PDF:/Users/savasp/Zotero/storage/XYRPRRLD/Jang et al. - 2024 - Mixed-Session Conversation with Egocentric Memory.pdf:application/pdf},
}

@inproceedings{yue_fragrel_2024,
	address = {Bangkok, Thailand},
	title = {{FragRel}: {Exploiting} {Fragment}-level {Relations} in the {External} {Memory} of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.findings-acl.968/},
	doi = {10.18653/v1/2024.findings-acl.968},
	abstract = {To process contexts with unlimited length using Large Language Models (LLMs), recent studies explore hierarchically managing the long text. Only several text fragments are taken from the external memory and passed into the temporary working memory, i.e., LLM`s context window. However, existing approaches isolatedly handle the text fragments without considering their structural connections, thereby suffering limited capability on texts with intensive inter-relations, e.g., coherent stories and code repositories. This work attempts to resolve this by exploiting the fragment-level relations in external memory. First, we formulate the fragment-level relations and present several instantiations for different text types. Next, we introduce a relation-aware fragment assessment criteria upon previous independent fragment assessment. Finally, we present the fragment-connected Hierarchical Memory based LLM. We validate the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Yue, Xihang and Zhu, Linchao and Yang, Yi},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {16348--16361},
	file = {PDF:/Users/savasp/Zotero/storage/24U5G5YF/Yue et al. - 2024 - FragRel Exploiting Fragment-level Relations in the External Memory of Large Language Models.pdf:application/pdf},
}

@inproceedings{wang_ldm2_2023,
	address = {Singapore},
	title = {{LDM}{\textasciicircum}2: {A} {Large} {Decision} {Model} {Imitating} {Human} {Cognition} with {Dynamic} {Memory} {Enhancement}},
	url = {https://aclanthology.org/2023.findings-emnlp.309/},
	doi = {10.18653/v1/2023.findings-emnlp.309},
	abstract = {With the rapid development of large language models (LLMs), it is highly demanded that LLMs can be adopted to make decisions to enable the artificial general intelligence. Most approaches leverage manually crafted examples to prompt the LLMs to imitate the decision process of human. However, designing optimal prompts is difficult and the patterned prompts can hardly be generalized to more complex environments. In this paper, we propose a novel model named Large Decision Model with Memory (LDM{\textasciicircum}2), which leverages a dynamic memory mechanism to construct dynamic prompts, guiding the LLMs in making proper decisions according to the faced state. LDM{\textasciicircum}2 consists of two stages: memory formation and memory refinement. In the former stage, human behaviors are decomposed into state-action tuples utilizing the powerful summarizing ability of LLMs. Then, these tuples are stored in the memory, whose indices are generated by the LLMs, to facilitate the retrieval of the most relevant subset of memorized tuples based on the current state. In the latter stage, our LDM{\textasciicircum}2 employs tree exploration to discover more suitable decision processes and enrich the memory by adding valuable state-action tuples. The dynamic circle of exploration and memory enhancement provides LDM{\textasciicircum}2 a better understanding of the global environment. Extensive experiments conducted in two interactive environments have shown that our LDM{\textasciicircum}2 outperforms the baselines in terms of both score and success rate, which demonstrates its effectiveness.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Xingjin and Li, Linjing and Zeng, Daniel},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {4660--4681},
	file = {PDF:/Users/savasp/Zotero/storage/ZQ4BGPDR/Wang et al. - 2023 - LDM^2 A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement.pdf:application/pdf},
}

@inproceedings{moskvichev_narrativexl_2023,
	address = {Singapore},
	title = {{NarrativeXL}: a {Large}-scale {Dataset} for {Long}-{Term} {Memory} {Models}},
	url = {https://aclanthology.org/2023.findings-emnlp.1005/},
	doi = {10.18653/v1/2023.findings-emnlp.1005},
	abstract = {We propose a new large-scale (nearly a million questions) ultra-long-context (more than 50,000 words average document length) reading comprehension dataset. Using GPT 3.5, we summarized each scene in 1,500 hand-curated fiction books from Project Gutenberg, which resulted in approximately 150 scene-level summaries per book. After that, we created a number of reading comprehension questions based on these summaries, including three types of multiple-choice scene recognition questions, as well as free-form narrative reconstruction questions. With 990,595 total questions, our dataset is an order of magnitude larger than the closest alternatives. Crucially, most questions have a known “retention demand”, indicating how long-term of a memory is needed to answer them, which should aid long-term memory performance evaluation. We validate our data in four small-scale experiments: one with human labelers, and three with existing language models. We show that our questions 1) adequately represent the source material 2) can be used to diagnose a model`s memory capacity 3) are not trivial for modern language models even when the memory demand does not exceed those models' context lengths. Lastly, we provide our code which can be used to further expand the dataset with minimal human labor.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Moskvichev, Arsenii and Mai, Ky-Vinh},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {15058--15072},
	file = {PDF:/Users/savasp/Zotero/storage/GYJSZSXV/Moskvichev and Mai - 2023 - NarrativeXL a Large-scale Dataset for Long-Term Memory Models.pdf:application/pdf},
}

@inproceedings{zhang_extractive_2024,
	address = {Miami, Florida, USA},
	title = {Extractive {Medical} {Entity} {Disambiguation} with {Memory} {Mechanism} and {Memorized} {Entity} {Information}},
	url = {https://aclanthology.org/2024.findings-emnlp.810/},
	doi = {10.18653/v1/2024.findings-emnlp.810},
	abstract = {Medical entity disambiguation (MED) aims to ground medical mentions in text with ontological entities in knowledge bases (KBs). A notable challenge of MED is the long medical text usually contains multiple entities' mentions with intricate correlations. However, limited by computation overhead, many existing methods consider only a single candidate entity mention during the disambiguation process. As such, they focus only on local MED optimal while ignoring the sole-mention disambiguation possibly boosted by richer context from other mentions' disambiguating processes – missing global optimal on entity combination in the text. Motivated by this, we propose a new approach called Extractive Medical Entity Disambiguation with Memory Mechanism and Memorized Entity Information (M3E). Specifically, we reformulate MED as a text extraction task, which simultaneously accepts the context of medical mentions, all possible candidate entities, and entity definitions, and it is then trained to extract the text span corresponding to the correct entity. Upon our new formulation, 1) to alleviate the computation overhead from the enriched context, we devise a memory mechanism module that performs memory caching, retrieval, fusion and cross-network residual; and 2) to utilize the disambiguation clues from other mentions, we design an auxiliary disambiguation module that employs a gating mechanism to assist the disambiguation of remaining mentions. Extensive experiments on two benchmark datasets demonstrate the superiority of M3E over the state-of-the-art MED methods on all metrics.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Guobiao and Peng, Xueping and Shen, Tao and Long, Guodong and Si, Jiasheng and Qin, Libo and Lu, Wenpeng},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {13811--13822},
	file = {PDF:/Users/savasp/Zotero/storage/QFSDFSC7/Zhang et al. - 2024 - Extractive Medical Entity Disambiguation with Memory Mechanism and Memorized Entity Information.pdf:application/pdf},
}

@misc{rasmussen_zep_2025,
	title = {Zep: {A} {Temporal} {Knowledge} {Graph} {Architecture} for {Agent} {Memory}},
	shorttitle = {Zep},
	url = {http://arxiv.org/abs/2501.13956},
	doi = {10.48550/arXiv.2501.13956},
	abstract = {We introduce Zep, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark. Additionally, Zep excels in more comprehensive and challenging evaluations than DMR that better reflect real-world enterprise use cases. While existing retrieval-augmented generation (RAG) frameworks for large language model (LLM)-based agents are limited to static document retrieval, enterprise applications demand dynamic knowledge integration from diverse sources including ongoing conversations and business data. Zep addresses this fundamental limitation through its core component Graphiti -- a temporally-aware knowledge graph engine that dynamically synthesizes both unstructured conversational data and structured business data while maintaining historical relationships. In the DMR benchmark, which the MemGPT team established as their primary evaluation metric, Zep demonstrates superior performance (94.8\% vs 93.4\%). Beyond DMR, Zep's capabilities are further validated through the more challenging LongMemEval benchmark, which better reflects enterprise use cases through complex temporal reasoning tasks. In this evaluation, Zep achieves substantial results with accuracy improvements of up to 18.5\% while simultaneously reducing response latency by 90\% compared to baseline implementations. These results are particularly pronounced in enterprise-critical tasks such as cross-session information synthesis and long-term context maintenance, demonstrating Zep's effectiveness for deployment in real-world applications.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Rasmussen, Preston and Paliychuk, Pavlo and Beauvais, Travis and Ryan, Jack and Chalef, Daniel},
	month = jan,
	year = {2025},
	note = {arXiv:2501.13956 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/YF9J463B/Rasmussen et al. - 2025 - Zep A Temporal Knowledge Graph Architecture for Agent Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/LARMBFLT/2501.html:text/html},
}

@misc{kim_ever-evolving_2024,
	title = {Ever-{Evolving} {Memory} by {Blending} and {Refining} the {Past}},
	url = {http://arxiv.org/abs/2403.04787},
	doi = {10.48550/arXiv.2403.04787},
	abstract = {For a human-like chatbot, constructing a long-term memory is crucial. However, current large language models often lack this capability, leading to instances of missing important user information or redundantly asking for the same information, thereby diminishing conversation quality. To effectively construct memory, it is crucial to seamlessly connect past and present information, while also possessing the ability to forget obstructive information. To address these challenges, we propose CREEM, a novel memory system for long-term conversation. Improving upon existing approaches that construct memory based solely on current sessions, CREEM blends past memories during memory formation. Additionally, we introduce a refining process to handle redundant or outdated information. Unlike traditional paradigms, we view responding and memory construction as inseparable tasks. The blending process, which creates new memories, also serves as a reasoning step for response generation by informing the connection between past and present. Through evaluation, we demonstrate that CREEM enhances both memory and response qualities in multi-session personalized dialogues.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Kim, Seo Hyun and Ka, Keummin and Jo, Yohan and Hwang, Seung-won and Lee, Dongha and Yeo, Jinyoung},
	month = apr,
	year = {2024},
	note = {arXiv:2403.04787 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/FZDEQ6HW/Kim et al. - 2024 - Ever-Evolving Memory by Blending and Refining the Past.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/N9KFC4YS/2403.html:text/html},
}

@misc{zheng_synapse_2024,
	title = {Synapse: {Trajectory}-as-{Exemplar} {Prompting} with {Memory} for {Computer} {Control}},
	shorttitle = {Synapse},
	url = {http://arxiv.org/abs/2306.07863},
	doi = {10.48550/arXiv.2306.07863},
	abstract = {Building agents with large language models (LLMs) for computer control is a burgeoning research area, where the agent receives computer states and performs actions to complete complex tasks. Previous computer agents have demonstrated the benefits of in-context learning (ICL); however, their performance is hindered by several issues. First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exemplars and overlook the similarity among tasks, resulting in poor generalization to novel tasks. To address these challenges, we introduce Synapse, a computer agent featuring three key components: i) state abstraction, which filters out task-irrelevant information from raw states, allowing more exemplars within the limited context, ii) trajectory-as-exemplar prompting, which prompts the LLM with complete trajectories of the abstracted states and actions to improve multi-step decision-making, and iii) exemplar memory, which stores the embeddings of exemplars and retrieves them via similarity search for generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse achieves a 99.2\% average success rate (a 10\% relative improvement) across 64 tasks using demonstrations from only 48 tasks. Notably, Synapse is the first ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a 56\% relative improvement in average step success rate over the previous state-of-the-art prompting scheme in Mind2Web.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Zheng, Longtao and Wang, Rundong and Wang, Xinrun and An, Bo},
	month = jan,
	year = {2024},
	note = {arXiv:2306.07863 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/MN3KLWWL/Zheng et al. - 2024 - Synapse Trajectory-as-Exemplar Prompting with Memory for Computer Control.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/S6MTY4Q4/2306.html:text/html},
}

@inproceedings{song_moviechat_2024,
	title = {{MovieChat}: {From} {Dense} {Token} to {Sparse} {Memory} for {Long} {Video} {Understanding}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and Lu, Yan and Hwang, Jenq-Neng and Wang, Gaoang},
	month = jun,
	year = {2024},
	pages = {18221--18232},
	file = {PDF:/Users/savasp/Zotero/storage/QIX5NGLE/Song et al. - 2024 - MovieChat From Dense Token to Sparse Memory for Long Video Understanding.pdf:application/pdf},
}

@misc{gutierrez_rag_2025,
	title = {From {RAG} to {Memory}: {Non}-{Parametric} {Continual} {Learning} for {Large} {Language} {Models}},
	shorttitle = {From {RAG} to {Memory}},
	url = {http://arxiv.org/abs/2502.14802},
	doi = {10.48550/arXiv.2502.14802},
	abstract = {Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7\% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Gutiérrez, Bernal Jiménez and Shu, Yiheng and Qi, Weijian and Zhou, Sizhe and Su, Yu},
	month = feb,
	year = {2025},
	note = {arXiv:2502.14802 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/EI4T2MB9/Gutiérrez et al. - 2025 - From RAG to Memory Non-Parametric Continual Learning for Large Language Models.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/TICYR3TL/2502.html:text/html},
}
