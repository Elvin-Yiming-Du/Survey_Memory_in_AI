
@inproceedings{yan_memory-aligned_2022,
	address = {Dublin, Ireland},
	title = {Memory-aligned {Knowledge} {Graph} for {Clinically} {Accurate} {Radiology} {Image} {Report} {Generation}},
	url = {https://aclanthology.org/2022.bionlp-1.11/},
	doi = {10.18653/v1/2022.bionlp-1.11},
	abstract = {Automatic generating the clinically accurate radiology report from X-ray images is important but challenging. The identification of multi-grained abnormal regions in image and corresponding abnormalities is difficult for data-driven neural models. In this work, we introduce a Memory-aligned Knowledge Graph (MaKG) of clinical abnormalities to better learn the visual patterns of abnormalities and their relationships by integrating it into a deep model architecture for the report generation. We carry out extensive experiments and show that the proposed MaKG deep model can improve the clinical accuracy of the generated reports.},
	booktitle = {Proceedings of the 21st {Workshop} on {Biomedical} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yan, Sixing},
	editor = {Demner-Fushman, Dina and Cohen, Kevin Bretonnel and Ananiadou, Sophia and Tsujii, Junichi},
	month = may,
	year = {2022},
	pages = {116--122},
	file = {PDF:/Users/savasp/Zotero/storage/GZ4B35DL/Yan - 2022 - Memory-aligned Knowledge Graph for Clinically Accurate Radiology Image Report Generation.pdf:application/pdf},
}

@inproceedings{lee_matter_2024,
	address = {Bangkok, Thailand},
	title = {{MATTER}: {Memory}-{Augmented} {Transformer} {Using} {Heterogeneous} {Knowledge} {Sources}},
	url = {https://aclanthology.org/2024.findings-acl.953/},
	doi = {10.18653/v1/2024.findings-acl.953},
	abstract = {Leveraging external knowledge is crucial for achieving high performance in knowledge-intensive tasks, such as question answering. The retrieve-and-read approach is widely adopted for integrating external knowledge into a language model. However, this approach suffers from increased computational cost and latency due to the long context length, which grows proportionally with the number of retrieved knowledge. Furthermore, existing retrieval-augmented models typically retrieve information from a single type of knowledge source, limiting their scalability to diverse knowledge sources with varying structures. In this work, we introduce an efficient memory-augmented transformer called MATTER, designed to retrieve relevant knowledge from multiple heterogeneous knowledge sources. Specifically, our model retrieves and reads from both unstructured sources (paragraphs) and semi-structured sources (QA pairs) in the form of fixed-length neural memories. We demonstrate that our model outperforms existing efficient retrieval-augmented models on popular QA benchmarks in terms of both accuracy and speed. Furthermore, MATTER achieves competitive results compared to conventional read-and-retrieve models while having 100x throughput during inference.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Dongkyu and Satya Prakash, Chandana and FitzGerald, Jack and Lehmann, Jens},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {16110--16121},
	file = {PDF:/Users/savasp/Zotero/storage/XA7AQKTP/Lee et al. - 2024 - MATTER Memory-Augmented Transformer Using Heterogeneous Knowledge Sources.pdf:application/pdf},
}

@misc{zeng_framework_2024,
	title = {A {Framework} for {Inference} {Inspired} by {Human} {Memory} {Mechanisms}},
	url = {http://arxiv.org/abs/2310.09297},
	doi = {10.48550/arXiv.2310.09297},
	abstract = {How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain extensive and complex relational knowledge and experience. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, reducing information conflicts and averting memory overflow. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretation of current perceptions. We exploratively apply our PMI to improve prevailing Transformers and CNN models on question-answering tasks like bAbI-20k and Sort-of-CLEVR datasets, as well as detecting equilateral triangles, language modeling and image classification tasks, and in each case, our PMI enhancements consistently outshine their original counterparts significantly. Visualization analyses reveal that relational memory consolidation, along with the interaction and integration of information from diverse memory sources, substantially contributes to the model effectiveness on inference tasks.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Zeng, Xiangyu and Lin, Jie and Hu, Piao and Huang, Ruizheng and Zhang, Zhicheng},
	month = may,
	year = {2024},
	note = {arXiv:2310.09297 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/2W8YZZ3I/Zeng et al. - 2024 - A Framework for Inference Inspired by Human Memory Mechanisms.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/H2G9BG5X/2310.html:text/html},
}

@misc{zou_m3_2025,
	title = {M3: {3D}-{Spatial} {MultiModal} {Memory}},
	shorttitle = {M3},
	url = {http://arxiv.org/abs/2503.16413},
	doi = {10.48550/arXiv.2503.16413},
	abstract = {We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3's feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Zou, Xueyan and Song, Yuchen and Qiu, Ri-Zhao and Peng, Xuanbin and Ye, Jianglong and Liu, Sifei and Wang, Xiaolong},
	month = mar,
	year = {2025},
	note = {arXiv:2503.16413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/TQZCGK6M/Zou et al. - 2025 - M3 3D-Spatial MultiModal Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/V5NWMF3I/2503.html:text/html},
}

@inproceedings{tian_learning_2024,
	address = {Bangkok, Thailand},
	title = {Learning {Multimodal} {Contrast} with {Cross}-modal {Memory} and {Reinforced} {Contrast} {Recognition}},
	url = {https://aclanthology.org/2024.findings-acl.391/},
	doi = {10.18653/v1/2024.findings-acl.391},
	abstract = {In many practical scenarios, contents from different modalities are not semantically aligned; for instance, visual and textual information may conflict with each other, resulting in non-compositional expression effects such as irony or humor. Effective modeling and smooth integration of multimodal information are crucial for achieving good understanding of the contrast across modalities. Being focusing on image-text matching, most current studies face challenges in identifying such contrast, leading to limitations in exploring the extended semantics when images and texts do not match. In this paper, we propose an LLM-based approach for learning multimodal contrast following the encoding-decoding paradigm, enhanced by a memory module with reinforced contrast recognition, and use a series of tasks that have the nature of multimodal contrast to verify our approach. The memory module learns the integration between visual and textual features with trainable memory vectors and the reinforced contrast recognition uses self-rejection sampling to optimize the memory to further enhance learning multimodal contrast. The resulted information, accompanied with visual and text features, is finally fed into the LLM to predict corresponding labels. We experiment our approach on four English and Chinese benchmark datasets, where it outperforms strong baselines and state-of-the-art studies.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Tian, Yuanhe and Xia, Fei and Song, Yan},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {6561--6573},
	file = {PDF:/Users/savasp/Zotero/storage/VGTAXL9K/Tian et al. - 2024 - Learning Multimodal Contrast with Cross-modal Memory and Reinforced Contrast Recognition.pdf:application/pdf},
}

@misc{song_moviechat_2024,
	title = {{MovieChat}+: {Question}-aware {Sparse} {Memory} for {Long} {Video} {Question} {Answering}},
	shorttitle = {{MovieChat}+},
	url = {http://arxiv.org/abs/2404.17176},
	doi = {10.48550/arXiv.2404.17176},
	abstract = {Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific pre-defined vision tasks. Yet, existing methods either employ complex spatial-temporal modules or rely heavily on additional perception models to extract temporal features for video understanding, and they only perform well on short videos. For long videos, the computational complexity and memory costs associated with long-term temporal connections are significantly increased, posing additional challenges.Taking advantage of the Atkinson-Shiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination with our specially designed memory mechanism, we propose MovieChat to overcome these challenges. We lift pre-trained multi-modal large language models for understanding long videos without incorporating additional trainable temporal modules, employing a zero-shot approach. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video, 2K temporal grounding labels, and 14K manual annotations for validation of the effectiveness of our method. The code along with the dataset can be accessed via the following https://github.com/rese1f/MovieChat.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Song, Enxin and Chai, Wenhao and Ye, Tian and Hwang, Jenq-Neng and Li, Xi and Wang, Gaoang},
	month = apr,
	year = {2024},
	note = {arXiv:2404.17176 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/W6ETIYK6/Song et al. - 2024 - MovieChat+ Question-aware Sparse Memory for Long Video Question Answering.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/BMA2NLTL/2404.html:text/html},
}

@misc{hu_chatdb_2023,
	title = {{ChatDB}: {Augmenting} {LLMs} with {Databases} as {Their} {Symbolic} {Memory}},
	shorttitle = {{ChatDB}},
	url = {http://arxiv.org/abs/2306.03901},
	doi = {10.48550/arXiv.2306.03901},
	abstract = {Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Hu, Chenxu and Fu, Jie and Du, Chenzhuang and Luo, Simian and Zhao, Junbo and Zhao, Hang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03901 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/9KWFJY98/Hu et al. - 2023 - ChatDB Augmenting LLMs with Databases as Their Symbolic Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/HTNWJ8SU/2306.html:text/html},
}

@misc{wang_delta_2025,
	title = {{DelTA}: {An} {Online} {Document}-{Level} {Translation} {Agent} {Based} on {Multi}-{Level} {Memory}},
	shorttitle = {{DelTA}},
	url = {http://arxiv.org/abs/2410.08143},
	doi = {10.48550/arXiv.2410.08143},
	abstract = {Large language models (LLMs) have achieved reasonable quality improvements in machine translation (MT). However, most current research on MT-LLMs still faces significant challenges in maintaining translation consistency and accuracy when processing entire documents. In this paper, we introduce DelTA, a Document-levEL Translation Agent designed to overcome these limitations. DelTA features a multi-level memory structure that stores information across various granularities and spans, including Proper Noun Records, Bilingual Summary, Long-Term Memory, and Short-Term Memory, which are continuously retrieved and updated by auxiliary LLM-based components. Experimental results indicate that DelTA significantly outperforms strong baselines in terms of translation consistency and quality across four open/closed-source LLMs and two representative document translation datasets, achieving an increase in consistency scores by up to 4.58 percentage points and in COMET scores by up to 3.16 points on average. DelTA employs a sentence-by-sentence translation strategy, ensuring no sentence omissions and offering a memory-efficient solution compared to the mainstream method. Furthermore, DelTA improves pronoun and context-dependent translation accuracy, and the summary component of the agent also shows promise as a tool for query-based summarization tasks. The code and data of our approach are released at https://github.com/YutongWang1216/DocMTAgent.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Wang, Yutong and Zeng, Jiali and Liu, Xuebo and Wong, Derek F. and Meng, Fandong and Zhou, Jie and Zhang, Min},
	month = mar,
	year = {2025},
	note = {arXiv:2410.08143 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/DTZ9877D/Wang et al. - 2025 - DelTA An Online Document-Level Translation Agent Based on Multi-Level Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/GXY459U7/2410.html:text/html},
}

@inproceedings{wu_efficient_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {An {Efficient} {Memory}-{Augmented} {Transformer} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {https://aclanthology.org/2022.emnlp-main.346/},
	doi = {10.18653/v1/2022.emnlp-main.346},
	abstract = {Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge source. Parametric and retrieval-augmented models have complementary strengths in terms of computational efficiency and predictive accuracy. To combine the strength of both approaches, we propose the Efficient Memory-Augmented Transformer (EMAT) – it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying. We also introduce pre-training tasks that allow EMAT to encode informative key-value representations, and to learn an implicit strategy to integrate multiple memory slots into the transformer. Experiments on various knowledge-intensive tasks such as question answering and dialogue datasets show that, simply augmenting parametric models (T5-base) using our method produces more accurate results (e.g., 25.8 → 44.3 EM on NQ) while retaining a high throughput (e.g., 1000 queries/s on NQ). Compared to retrieval-augmented models, EMAT runs substantially faster across the board and produces more accurate results on WoW and ELI5.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Yuxiang and Zhao, Yu and Hu, Baotian and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {5184--5196},
	file = {PDF:/Users/savasp/Zotero/storage/Y2F4BGKS/Wu et al. - 2022 - An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks.pdf:application/pdf},
}

@inproceedings{wang_symbolic_2024,
	address = {Miami, Florida, USA},
	title = {Symbolic {Working} {Memory} {Enhances} {Language} {Models} for {Complex} {Rule} {Application}},
	url = {https://aclanthology.org/2024.emnlp-main.974/},
	doi = {10.18653/v1/2024.emnlp-main.974},
	abstract = {Large Language Models (LLMs) have shown remarkable reasoning performance but struggle with multi-step deductive reasoning involving a series of rule application steps, especially when rules are presented non-sequentially. Our preliminary analysis shows that while LLMs excel in single-step rule application, their performance drops significantly in multi-step scenarios due to the challenge in rule grounding. It requires anchoring the applicable rule and supporting facts at each step, amidst multiple input rules, facts, and inferred facts. To address this, we propose augmenting LLMs with external working memory and introduce a neurosymbolic framework for rule application. The memory stores facts and rules in both natural language and symbolic forms, enabling precise tracking. Utilizing this memory, our framework iteratively performs symbolic rule grounding and LLM-based rule implementation. The former matches predicates and variables of symbolic rules and facts to ground applicable rules at each step. Experiments indicate our framework`s effectiveness in rule application and its robustness across various steps and settings.},
	urldate = {2025-05-12},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Siyuan and Wei, Zhongyu and Choi, Yejin and Ren, Xiang},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {17583--17604},
	file = {Full Text PDF:/Users/savasp/Zotero/storage/4PEGAANF/Wang et al. - 2024 - Symbolic Working Memory Enhances Language Models for Complex Rule Application.pdf:application/pdf},
}

@inproceedings{sarch_open-ended_2023,
	address = {Singapore},
	title = {Open-{Ended} {Instructable} {Embodied} {Agents} with {Memory}-{Augmented} {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-emnlp.226/},
	doi = {10.18653/v1/2023.findings-emnlp.226},
	abstract = {Pre-trained and frozen LLMs can effectively map simple scene re-arrangement instructions to programs over a robot`s visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user`s idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user`s language and action plans, to assist future inferences and personalize them to the user`s language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with 1.7x improvement over the previous SOTA for TfD. Our models, code and video results can be found in our project`s website: https://helper-agent-llm.github.io.},
	urldate = {2025-05-12},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Sarch, Gabriel and Wu, Yue and Tarr, Michael and Fragkiadaki, Katerina},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {3468--3500},
	file = {Full Text PDF:/Users/savasp/Zotero/storage/W2LRJW7A/Sarch et al. - 2023 - Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models.pdf:application/pdf},
}

@inproceedings{fu_there_2022,
	address = {Dublin, Ireland},
	title = {There {Are} a {Thousand} {Hamlets} in a {Thousand} {People}`s {Eyes}: {Enhancing} {Knowledge}-grounded {Dialogue} with {Personal} {Memory}},
	shorttitle = {There {Are} a {Thousand} {Hamlets} in a {Thousand} {People}`s {Eyes}},
	url = {https://aclanthology.org/2022.acl-long.270/},
	doi = {10.18653/v1/2022.acl-long.270},
	abstract = {Knowledge-grounded conversation (KGC) shows great potential in building an engaging and knowledgeable chatbot, and knowledge selection is a key ingredient in it. However, previous methods for knowledge selection only concentrate on the relevance between knowledge and dialogue context, ignoring the fact that age, hobby, education and life experience of an interlocutor have a major effect on his or her personal preference over external knowledge. Without taking the personalization issue into account, it is difficult for existing dialogue systems to select the proper knowledge and generate persona-consistent responses. In this work, we introduce personal memory into knowledge selection in KGC to address the personalization issue. We propose a variational method to model the underlying relationship between one`s personal memory and his or her selection of knowledge, and devise a learning scheme in which the forward mapping from personal memory to knowledge and its inverse mapping is included in a closed loop so that they could teach each other. Experiment results show that our methods outperform existing KGC methods significantly on both automatic evaluation and human evaluation.},
	urldate = {2025-05-12},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fu, Tingchen and Zhao, Xueliang and Tao, Chongyang and Wen, Ji-Rong and Yan, Rui},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {3901--3913},
	file = {Full Text PDF:/Users/savasp/Zotero/storage/E8MPYDBG/Fu et al. - 2022 - There Are a Thousand Hamlets in a Thousand People`s Eyes Enhancing Knowledge-grounded Dialogue with.pdf:application/pdf},
}

@inproceedings{jin_prior_2022,
	address = {Dublin, Ireland},
	title = {Prior {Knowledge} and {Memory} {Enriched} {Transformer} for {Sign} {Language} {Translation}},
	url = {https://aclanthology.org/2022.findings-acl.297/},
	doi = {10.18653/v1/2022.findings-acl.297},
	abstract = {This paper attacks the challenging problem of sign language translation (SLT), which involves not only visual and textual understanding but also additional prior knowledge learning (i.e. performing style, syntax). However, the majority of existing methods with vanilla encoder-decoder structures fail to sufficiently explore all of them. Based on this concern, we propose a novel method called Prior knowledge and memory Enriched Transformer (PET) for SLT, which incorporates the auxiliary information into vanilla transformer. Concretely, we develop gated interactive multi-head attention which associates the multimodal representation and global signing style with adaptive gated functions. One Part-of-Speech (POS) sequence generator relies on the associated information to predict the global syntactic structure, which is thereafter leveraged to guide the sentence generation. Besides, considering that the visual-textual context information, and additional auxiliary knowledge of a word may appear in more than one video, we design a multi-stream memory structure to obtain higher-quality translations, which stores the detailed correspondence between a word and its various relevant information, leading to a more comprehensive understanding for each word. We conduct extensive empirical studies on RWTH-PHOENIX-Weather-2014 dataset with both signer-dependent and signer-independent conditions. The quantitative and qualitative experimental results comprehensively reveal the effectiveness of PET.},
	urldate = {2025-05-12},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Jin, Tao and Zhao, Zhou and Zhang, Meng and Zeng, Xingshan},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {3766--3775},
	file = {Full Text PDF:/Users/savasp/Zotero/storage/5G6S9JC3/Jin et al. - 2022 - Prior Knowledge and Memory Enriched Transformer for Sign Language Translation.pdf:application/pdf},
}

@inproceedings{wan_g-map_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {G-{MAP}: {General} {Memory}-{Augmented} {Pre}-trained {Language} {Model} for {Domain} {Tasks}},
	shorttitle = {G-{MAP}},
	url = {https://aclanthology.org/2022.emnlp-main.441/},
	doi = {10.18653/v1/2022.emnlp-main.441},
	abstract = {General pre-trained language models (PLMs), such as BERT, have achieved remarkable performance on various NLP tasks. Recently, domain-specific PLMs have been proposed to boost the task performance of specific domains (e.g., biomedical and computer science) by continuing to pre-train general PLMs with domain-specific corpora. However, this domain-adaptive pre-training (DAPT (CITATION)) tends to forget the previous general knowledge acquired by general PLMs, which leads to a catastrophic forgetting phenomenon and sub-optimal performance. To alleviate this problem, we propose a new framework of Memory-Augmented Pre-trained Language Model (MAP), which augments the domain-specific PLM by a memory built from the frozen general PLM without losing the general knowledge. Specifically, we propose a new memory-augmented layer, and based on it, different augmentation strategies are explored to build memory and fusion memory into domain-specific PLM. We demonstrate the effectiveness of MAP on different domains (biomedical and computer science publications, news, and reviews) and different kinds (text classification, QA, NER) of tasks, and the extensive results show that the proposed MAP can achieve SOTA results on these tasks.},
	urldate = {2025-05-12},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wan, Zhongwei and Yin, Yichun and Zhang, Wei and Shi, Jiaxin and Shang, Lifeng and Chen, Guangyong and Jiang, Xin and Liu, Qun},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {6585--6597},
	file = {Full Text PDF:/Users/savasp/Zotero/storage/ZVZ69SIM/Wan et al. - 2022 - G-MAP General Memory-Augmented Pre-trained Language Model for Domain Tasks.pdf:application/pdf},
}

@inproceedings{nogueira_dos_santos_memory_2024,
	address = {Mexico City, Mexico},
	title = {Memory {Augmented} {Language} {Models} through {Mixture} of {Word} {Experts}},
	url = {https://aclanthology.org/2024.naacl-long.249/},
	doi = {10.18653/v1/2024.naacl-long.249},
	abstract = {Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to aggressively decouple learning capacity and FLOPs through Mixture-of-Experts (MoE) style models with large knowledge-rich vocabulary based routing functions. Our proposed approach, dubbed Mixture of Word Experts (MoWE), can be seen as a memory augmented model, where a large set of word-specific experts play the role of a sparse memory. We demonstrate that MoWE performs significantly better than the T5 family of models with similar number of FLOPs in a variety of NLP tasks. Moreover, MoWE outperforms traditional MoE models on knowledge intensive tasks and has similar performance to complex memory augmented approaches that often require to invoke custom mechanisms to search the sparse memory.},
	urldate = {2025-05-12},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Nogueira dos Santos, Cicero and Lee-Thorp, James and Noble, Isaac and Chang, Chung-Ching and Uthus, David},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {4425--4438},
	file = {Full Text PDF:/Users/savasp/Zotero/storage/9WDDANWL/Nogueira dos Santos et al. - 2024 - Memory Augmented Language Models through Mixture of Word Experts.pdf:application/pdf},
}

@inproceedings{pei_cooperative_2021,
	address = {New York, NY, USA},
	series = {{WWW} '21},
	title = {A {Cooperative} {Memory} {Network} for {Personalized} {Task}-oriented {Dialogue} {Systems} with {Incomplete} {User} {Profiles}},
	isbn = {978-1-4503-8312-7},
	url = {https://doi.org/10.1145/3442381.3449843},
	doi = {10.1145/3442381.3449843},
	abstract = {There is increasing interest in developing personalized Task-oriented Dialogue Systems (TDSs). Previous work on personalized TDSs often assumes that complete user profiles are available for most or even all users. This is unrealistic because In this paper, we study personalized TDSs without assuming that user profiles are complete. We propose a Cooperative Memory Network (CoMemNN) that has a novel mechanism to gradually enrich user profiles as dialogues progress and to simultaneously improve response selection based on the enriched profiles. Cooperative Memory Network (CoMemNN) consists of two core modules: User Profile Enrichment (UPE) and Dialogue Response Selection (DRS). The former enriches incomplete user profiles by utilizing collaborative information from neighbor users as well as current dialogues. The latter uses the enriched profiles to update the current user query so as to encode more useful information, based on which a personalized response to a user request is selected. We conduct extensive experiments on the personalized bAbI dialogue benchmark datasets. We find that CoMemNN is able to enrich user profiles effectively, which results in an improvement of 3.06\% in terms of response selection accuracy compared to state-of-the-art methods. We also test the robustness of CoMemNN against incompleteness of user profiles by randomly discarding attribute values from user profiles. Even when discarding 50\% of the attribute values, CoMemNN is able to match the performance of the best performing baseline without discarding user profiles, showing the robustness of CoMemNN.},
	urldate = {2025-05-12},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Pei, Jiahuan and Ren, Pengjie and de Rijke, Maarten},
	month = jun,
	year = {2021},
	pages = {1552--1561},
	file = {Full Text:/Users/savasp/Zotero/storage/D52V43XP/Pei et al. - 2021 - A Cooperative Memory Network for Personalized Task-oriented Dialogue Systems with Incomplete User Pr.pdf:application/pdf},
}
