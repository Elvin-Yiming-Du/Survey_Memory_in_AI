
@inproceedings{meng_mass-editing_2023,
	title = {Mass-{Editing} {Memory} in a {Transformer}},
	url = {https://openreview.net/forum?id=MkbcAHIYgyS},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex J. and Belinkov, Yonatan and Bau, David},
	year = {2023},
	file = {PDF:/Users/savasp/Zotero/storage/GZ5Z5MU5/Meng et al. - 2023 - Mass-Editing Memory in a Transformer.pdf:application/pdf},
}

@inproceedings{mitchell_memory-based_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Memory-{Based} {Model} {Editing} at {Scale}},
	volume = {162},
	url = {https://proceedings.mlr.press/v162/mitchell22a.html},
	abstract = {Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit’s intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model’s predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	month = jul,
	year = {2022},
	pages = {15817--15831},
}

@inproceedings{madaan_memory-assisted_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Memory-assisted prompt editing to improve {GPT}-3 after deployment},
	url = {https://aclanthology.org/2022.emnlp-main.183/},
	doi = {10.18653/v1/2022.emnlp-main.183},
	abstract = {Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret “What word is similar to good?” to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user`s intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {2833--2861},
	file = {PDF:/Users/savasp/Zotero/storage/RAH8E8XA/Madaan et al. - 2022 - Memory-assisted prompt editing to improve GPT-3 after deployment.pdf:application/pdf},
}

@inproceedings{zhong_training_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Training {Language} {Models} with {Memory} {Augmentation}},
	url = {https://aclanthology.org/2022.emnlp-main.382/},
	doi = {10.18653/v1/2022.emnlp-main.382},
	abstract = {Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories—local, long-term, and external memory—at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhong, Zexuan and Lei, Tao and Chen, Danqi},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {5657--5673},
	file = {PDF:/Users/savasp/Zotero/storage/DYEJ9VCX/Zhong et al. - 2022 - Training Language Models with Memory Augmentation.pdf:application/pdf},
}

@inproceedings{kim_efficient_2020,
	address = {Online},
	title = {Efficient {Dialogue} {State} {Tracking} by {Selectively} {Overwriting} {Memory}},
	url = {https://aclanthology.org/2020.acl-main.53/},
	doi = {10.18653/v1/2020.acl-main.53},
	abstract = {Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72\% in MultiWOZ 2.0 and 53.01\% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Sungdong and Yang, Sohee and Kim, Gyuwan and Lee, Sang-Woo},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {567--582},
	file = {PDF:/Users/savasp/Zotero/storage/87KTHPAQ/Kim et al. - 2020 - Efficient Dialogue State Tracking by Selectively Overwriting Memory.pdf:application/pdf},
}

@inproceedings{tamayo_mass-editing_2024,
	address = {Bangkok, Thailand},
	title = {Mass-{Editing} {Memory} with {Attention} in {Transformers}: {A} cross-lingual exploration of knowledge},
	url = {https://aclanthology.org/2024.findings-acl.347/},
	doi = {10.18653/v1/2024.findings-acl.347},
	abstract = {Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10\% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at https://github.com/dtamayo-nlp/MEMAT.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Tamayo, Daniel and Gonzalez-Agirre, Aitor and Hernando, Javier and Villegas, Marta},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {5831--5847},
	file = {PDF:/Users/savasp/Zotero/storage/BHYYP4ZA/Tamayo et al. - 2024 - Mass-Editing Memory with Attention in Transformers A cross-lingual exploration of knowledge.pdf:application/pdf},
}

@inproceedings{mehta_dsi_2023,
	address = {Singapore},
	title = {{DSI}++: {Updating} {Transformer} {Memory} with {New} {Documents}},
	url = {https://aclanthology.org/2023.emnlp-main.510/},
	doi = {10.18653/v1/2023.emnlp-main.510},
	abstract = {Differentiable Search Indices (DSIs) encode a corpus of documents in the parameters of a model and use the same model to map queries directly to relevant document identifiers. Despite the solid performance of DSI models, successfully deploying them in scenarios where document corpora change with time is an open problem. In this work, we introduce DSI++, a continual learning challenge for DSI with the goal of continuously indexing new documents while being able to answer queries related to both previously and newly indexed documents. Across different model scales and document identifier representations, we show that continual indexing of new documents leads to considerable forgetting of previously indexed documents. We also hypothesize and verify that the model experiences forgetting events during training, leading to unstable learning. To mitigate these issues, we investigate two approaches. The first focuses on modifying the training dynamics. Flatter minima implicitly alleviates forgetting, so we explicitly optimize for flatter loss basins and show that the model stably memorizes more documents (+12\%). Next, we introduce a parametric memory to generate pseudo-queries for documents and supplement them during incremental indexing to prevent forgetting for the retrieval task. Extensive experiments on a novel continual indexing benchmark based on Natural Questions demonstrate that our proposed solution mitigates the forgetting in DSI++ by a significant margin and improves the average Hits@10 by +21.1\% over competitive baselines.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mehta, Sanket Vaibhav and Gupta, Jai and Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Rao, Jinfeng and Najork, Marc and Strubell, Emma and Metzler, Donald},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {8198--8213},
	file = {PDF:/Users/savasp/Zotero/storage/SSE97T3W/Mehta et al. - 2023 - DSI++ Updating Transformer Memory with New Documents.pdf:application/pdf},
}

@article{yang_memory3_2024,
	title = {Memory3: {Language} {Modeling} with {Explicit} {Memory}},
	volume = {3},
	issn = {2790-2048, 2790-203X},
	shorttitle = {\${\textbackslash}text\{{Memory}\}{\textasciicircum}3\$},
	url = {http://arxiv.org/abs/2407.01178},
	doi = {10.4208/jml.240708},
	abstract = {The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining "abstract knowledge". As a preliminary proof of concept, we train from scratch a 2.4B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named \${\textbackslash}text\{Memory\}{\textasciicircum}3\$, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation.},
	number = {3},
	urldate = {2025-05-12},
	journal = {Journal of Machine Learning},
	author = {Yang, Hongkang and Lin, Zehao and Wang, Wenjin and Wu, Hao and Li, Zhiyu and Tang, Bo and Wei, Wenqiang and Wang, Jinbo and Tang, Zeyun and Song, Shichao and Xi, Chenyang and Yu, Yu and Chen, Kai and Xiong, Feiyu and Tang, Linpeng and E, Weinan},
	month = jan,
	year = {2024},
	note = {arXiv:2407.01178 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {300--346},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/ZVW4TUNC/Yang et al. - 2024 - \$text Memory ^3\$ Language Modeling with Explicit Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/WR9FXW2K/2407.html:text/html},
}

@misc{sun_information-theoretic_2022,
	title = {Information-theoretic {Online} {Memory} {Selection} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/2204.04763},
	doi = {10.48550/arXiv.2204.04763},
	abstract = {A challenging problem in task-free continual learning is the online selection of a representative replay memory from data streams. In this work, we investigate the online memory selection problem from an information-theoretic perspective. To gather the most information, we propose the {\textbackslash}textit\{surprise\} and the {\textbackslash}textit\{learnability\} criteria to pick informative points and to avoid outliers. We present a Bayesian model to compute the criteria efficiently by exploiting rank-one matrix structures. We demonstrate that these criteria encourage selecting informative points in a greedy algorithm for online memory selection. Furthermore, by identifying the importance of {\textbackslash}textit\{the timing to update the memory\}, we introduce a stochastic information-theoretic reservoir sampler (InfoRS), which conducts sampling among selective points with high information. Compared to reservoir sampling, InfoRS demonstrates improved robustness against data imbalance. Finally, empirical performances over continual learning benchmarks manifest its efficiency and efficacy.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Sun, Shengyang and Calandriello, Daniele and Hu, Huiyi and Li, Ang and Titsias, Michalis},
	month = apr,
	year = {2022},
	note = {arXiv:2204.04763 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/RJGQTZAS/Sun et al. - 2022 - Information-theoretic Online Memory Selection for Continual Learning.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/SW6FY8Y4/2204.html:text/html},
}

@inproceedings{wang_improving_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Improving {Task}-free {Continual} {Learning} by {Distributionally} {Robust} {Memory} {Evolution}},
	volume = {162},
	url = {https://proceedings.mlr.press/v162/wang22v.html},
	abstract = {Task-free continual learning (CL) aims to learn a non-stationary data stream without explicit task definitions and not forget previous knowledge. The widely adopted memory replay approach could gradually become less effective for long data streams, as the model may memorize the stored examples and overfit the memory buffer. Second, existing methods overlook the high uncertainty in the memory data distribution since there is a big gap between the memory data distribution and the distribution of all the previous data examples. To address these problems, for the first time, we propose a principled memory evolution framework to dynamically evolve the memory data distribution by making the memory buffer gradually harder to be memorized with distributionally robust optimization (DRO). We then derive a family of methods to evolve the memory buffer data in the continuous probability measure space with Wasserstein gradient flow (WGF). The proposed DRO is w.r.t the worst-case evolved memory data distribution, thus guarantees the model performance and learns significantly more robust features than existing memory-replay-based methods. Extensive experiments on existing benchmarks demonstrate the effectiveness of the proposed methods for alleviating forgetting. As a by-product of the proposed framework, our method is more robust to adversarial examples than existing task-free CL methods.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Zhenyi and Shen, Li and Fang, Le and Suo, Qiuling and Duan, Tiehang and Gao, Mingchen},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	month = jul,
	year = {2022},
	pages = {22985--22998},
}

@inproceedings{shi_unified_2023,
	title = {A {Unified} {Approach} to {Domain} {Incremental} {Learning} with {Memory}: {Theory} and {Algorithm}},
	url = {https://openreview.net/forum?id=FiClXlUqA7},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Shi, Haizhou and Wang, Hao},
	year = {2023},
	file = {PDF:/Users/savasp/Zotero/storage/XA8YRFLE/Shi and Wang - 2023 - A Unified Approach to Domain Incremental Learning with Memory Theory and Algorithm.pdf:application/pdf},
}

@inproceedings{farahani_deciphering_2024,
	address = {Miami, Florida, USA},
	title = {Deciphering the {Interplay} of {Parametric} and {Non}-parametric {Memory} in {Retrieval}-augmented {Language} {Models}},
	url = {https://aclanthology.org/2024.emnlp-main.943/},
	doi = {10.18653/v1/2024.emnlp-main.943},
	abstract = {Generative language models often struggle with specialized or less-discussed knowledge. A potential solution is found in Retrieval-Augmented Generation (RAG) models which act like retrieving information before generating responses. In this study, we explore how the Atlas approach, a RAG model, decides between what it already knows (parametric) and what it retrieves (non-parametric). We use causal mediation analysis and controlled experiments to examine how internal representations influence information processing. Our findings disentangle the effects of parametric knowledge and the retrieved context. They indicate that in cases where the model can choose between both types of information (parametric and non-parametric), it relies more on the context than the parametric knowledge. Furthermore, the analysis investigates the computations involved in \textit{how} the model uses the information from the context. We find that multiple mechanisms are active within the model and can be detected with mediation analysis: first, the decision of \textit{whether the context is relevant}, and second, how the encoder computes output representations to support copying when relevant.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Farahani, Mehrdad and Johansson, Richard},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {16966--16977},
	file = {PDF:/Users/savasp/Zotero/storage/NVHT2ZWB/Farahani and Johansson - 2024 - Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Mo.pdf:application/pdf},
}

@misc{bricken_sparse_2023,
	title = {Sparse {Distributed} {Memory} is a {Continual} {Learner}},
	url = {http://arxiv.org/abs/2303.11934},
	doi = {10.48550/arXiv.2303.11934},
	abstract = {Continual learning is a problem for artificial neural networks that their biological counterparts are adept at solving. Building on work using Sparse Distributed Memory (SDM) to connect a core neural circuit with the powerful Transformer model, we create a modified Multi-Layered Perceptron (MLP) that is a strong continual learner. We find that every component of our MLP variant translated from biology is necessary for continual learning. Our solution is also free from any memory replay or task information, and introduces novel methods to train sparse networks that may be broadly applicable.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Bricken, Trenton and Davies, Xander and Singh, Deepak and Krotov, Dmitry and Kreiman, Gabriel},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11934 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/QLT5ZRYV/Bricken et al. - 2023 - Sparse Distributed Memory is a Continual Learner.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/TGZGL64S/2303.html:text/html},
}

@inproceedings{zhao_improving_2022,
	address = {Dublin, Ireland},
	title = {Improving {Meta}-learning for {Low}-resource {Text} {Classification} and {Generation} via {Memory} {Imitation}},
	url = {https://aclanthology.org/2022.acl-long.44/},
	doi = {10.18653/v1/2022.acl-long.44},
	abstract = {Building models of natural language processing (NLP) is challenging in low-resource scenarios where limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks. To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model`s reliance on support sets for task adaptation. Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of support sets stored in the memory. A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Yingxiu and Tian, Zhiliang and Yao, Huaxiu and Zheng, Yinhe and Lee, Dongkyu and Song, Yiping and Sun, Jian and Zhang, Nevin},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {583--595},
	file = {PDF:/Users/savasp/Zotero/storage/2A4VFU4Z/Zhao et al. - 2022 - Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation.pdf:application/pdf},
}

@inproceedings{sharma_content_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Content {Addressable} {Memory} {Without} {Catastrophic} {Forgetting} by {Heteroassociation} with a {Fixed} {Scaffold}},
	volume = {162},
	url = {https://proceedings.mlr.press/v162/sharma22b.html},
	abstract = {Content-addressable memory (CAM) networks, so-called because stored items can be recalled by partial or corrupted versions of the items, exhibit near-perfect recall of a small number of information-dense patterns below capacity and a ’memory cliff’ beyond, such that inserting a single additional pattern results in catastrophic loss of all stored patterns. We propose a novel CAM architecture, Memory Scaffold with Heteroassociation (MESH), that factorizes the problems of internal attractor dynamics and association with external content to generate a CAM continuum without a memory cliff: Small numbers of patterns are stored with complete information recovery matching standard CAMs, while inserting more patterns still results in partial recall of every pattern, with a graceful trade-off between pattern number and pattern richness. Motivated by the architecture of the Entorhinal-Hippocampal memory circuit in the brain, MESH is a tripartite architecture with pairwise interactions that uses a predetermined set of internally stabilized states together with heteroassociation between the internal states and arbitrary external patterns. We show analytically and experimentally that for any number of stored patterns, MESH nearly saturates the total information bound (given by the number of synapses) for CAM networks, outperforming all existing CAM models.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sharma, Sugandha and Chandra, Sarthak and Fiete, Ila},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	month = jul,
	year = {2022},
	pages = {19658--19682},
}

@inproceedings{mhamdi_leitner-guided_2024,
	address = {Mexico City, Mexico},
	title = {Leitner-{Guided} {Memory} {Replay} for {Cross}-lingual {Continual} {Learning}},
	url = {https://aclanthology.org/2024.naacl-long.432/},
	doi = {10.18653/v1/2024.naacl-long.432},
	abstract = {Cross-lingual continual learning aims to continuously fine-tune a downstream model on emerging data from new languages. One major challenge in cross-lingual continual learning is catastrophic forgetting: a stability-plasticity dilemma, where performance on previously seen languages decreases as the model learns to transfer to new languages. Experience replay, which revisits data from a fixed-size memory of old languages while training on new ones, is among the most successful approaches for solving this dilemma. Faced with the challenge of dynamically storing the memory with high-quality examples while complying with its fixed size limitations, we consider Leitner queuing, a human-inspired spaced-repetition technique, to determine what should be replayed at each phase of learning. Via a controlled set of quantitative and qualitative analyses across different memory strategies, we show that, just like humans, carefully picking informative examples to be prioritized in cross-lingual memory replay helps tame the stability-plasticity dilemma. Compared to vanilla and strong memory replay baselines, our Leitner-guided approach significantly and consistently decreases forgetting while maintaining accuracy across natural language understanding tasks, language orders, and languages.},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {M'hamdi, Meryem and May, Jonathan},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {7808--7821},
	file = {PDF:/Users/savasp/Zotero/storage/5AJDP63Z/M'hamdi and May - 2024 - Leitner-Guided Memory Replay for Cross-lingual Continual Learning.pdf:application/pdf},
}

@inproceedings{liu_navigating_2022,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '22},
	title = {Navigating memory construction by global pseudo-task simulation for continual learning},
	isbn = {978-1-7138-7108-8},
	abstract = {Continual learning faces a crucial challenge of catastrophic forgetting. To address this challenge, experience replay (ER) that maintains a tiny subset of samples from previous tasks has been commonly used. Existing ER works usually focus on refining the learning objective for each task with a static memory construction policy. In this paper, we formulate the dynamic memory construction in ER as a combinatorial optimization problem, which aims at directly minimizing the global loss across all experienced tasks. We first apply three tactics to solve the problem in the offline setting as a starting point. To provide an approximate solution to this problem in the online continual learning setting, we further propose the Global Pseudo-task Simulation (GPS), which mimics future catastrophic forgetting of the current task by permutation. Our empirical results and analyses suggest that the GPS consistently improves accuracy across four commonly used vision benchmarks. We have also shown that our GPS can serve as the unified framework for integrating various memory construction policies in existing ER works.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Liu, Yejia and Zhu, Wang and Ren, Shaolei},
	year = {2022},
	note = {event-place: New Orleans, LA, USA},
	file = {PDF:/Users/savasp/Zotero/storage/QAKQGTLI/Liu et al. - 2022 - Navigating memory construction by global pseudo-task simulation for continual learning.pdf:application/pdf},
}

@inproceedings{kim_transformer_2023,
	title = {Transformer as a hippocampal memory consolidation model based on {NMDAR}-inspired nonlinearity},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/2f1eb4c897e63870eee9a0a0f7a10332-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kim, Dong Kyum and Kwon, Jea and Cha, Meeyoung and Lee, C.},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {14637--14664},
	file = {PDF:/Users/savasp/Zotero/storage/YPVB2GPA/Kim et al. - 2023 - Transformer as a hippocampal memory consolidation model based on NMDAR-inspired nonlinearity.pdf:application/pdf},
}

@article{kiley_mechanisms_2022,
	title = {Mechanisms of {Memory} {Updating}: {State} {Dependency} vs. {Reconsolidation}},
	volume = {5},
	issn = {2514-4820},
	shorttitle = {Mechanisms of {Memory} {Updating}},
	url = {http://www.journalofcognition.org/articles/10.5334/joc.198/},
	doi = {10.5334/joc.198},
	language = {en},
	number = {1},
	urldate = {2025-05-12},
	journal = {Journal of Cognition},
	author = {Kiley, Christopher and Parks, Colleen M.},
	month = jan,
	year = {2022},
	pages = {7},
	file = {Full Text PDF:/Users/savasp/Zotero/storage/49EJZ3I9/Kiley and Parks - 2022 - Mechanisms of Memory Updating State Dependency vs. Reconsolidation.pdf:application/pdf},
}

@misc{kang_think_2024,
	title = {Think {Before} {You} {Act}: {Decision} {Transformers} with {Working} {Memory}},
	shorttitle = {Think {Before} {You} {Act}},
	url = {http://arxiv.org/abs/2305.16338},
	doi = {10.48550/arXiv.2305.16338},
	abstract = {Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Kang, Jikun and Laroche, Romain and Yuan, Xingdi and Trischler, Adam and Liu, Xue and Fu, Jie},
	month = may,
	year = {2024},
	note = {arXiv:2305.16338 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/NN4UMV8L/Kang et al. - 2024 - Think Before You Act Decision Transformers with Working Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/DNQW78JU/2305.html:text/html},
}

@misc{chen_improving_2025,
	title = {Improving {Factuality} with {Explicit} {Working} {Memory}},
	url = {http://arxiv.org/abs/2412.18069},
	doi = {10.48550/arXiv.2412.18069},
	abstract = {Large language models can generate factually inaccurate content, a problem known as hallucination. Recent works have built upon retrieved-augmented generation to improve factuality through iterative prompting but these methods are limited by the traditional RAG design. To address these challenges, we introduce EWE (Explicit Working Memory), a novel approach that enhances factuality in long-form text generation by integrating a working memory that receives real-time feedback from external resources. The memory is refreshed based on online fact-checking and retrieval feedback, allowing EWE to rectify false claims during the generation process and ensure more accurate and reliable outputs. Our experiments demonstrate that Ewe outperforms strong baselines on four fact-seeking long-form generation datasets, increasing the factuality metric, VeriScore, by 2 to 6 points absolute without sacrificing the helpfulness of the responses. Further analysis reveals that the design of rules for memory updates, configurations of memory units, and the quality of the retrieval datastore are crucial factors for influencing model performance.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Chen, Mingda and Li, Yang and Padthe, Karthik and Shao, Rulin and Sun, Alicia and Zettlemoyer, Luke and Ghosh, Gargi and Yih, Wen-tau},
	month = feb,
	year = {2025},
	note = {arXiv:2412.18069 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/87CW9QGY/Chen et al. - 2025 - Improving Factuality with Explicit Working Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/EPCXPECM/2412.html:text/html},
}

@misc{fang_alphaedit_2025,
	title = {{AlphaEdit}: {Null}-{Space} {Constrained} {Knowledge} {Editing} for {Language} {Models}},
	shorttitle = {{AlphaEdit}},
	url = {http://arxiv.org/abs/2410.02355},
	doi = {10.48550/arXiv.2410.02355},
	abstract = {Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential parameters and then edits them by introducing a perturbation. While effective, current studies have demonstrated that this perturbation inevitably disrupt the originally preserved knowledge within LLMs, especially in sequential editing scenarios. To address this, we introduce AlphaEdit, a novel solution that projects perturbation onto the null space of the preserved knowledge before applying it to the parameters. We theoretically prove that this projection ensures the output of post-edited LLMs remains unchanged when queried about the preserved knowledge, thereby mitigating the issue of disruption. Extensive experiments on various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts the performance of most locating-then-editing methods by an average of 36.7\% with a single line of additional code for projection solely. Our code is available at: https://github.com/jianghoucheng/AlphaEdit.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Fang, Junfeng and Jiang, Houcheng and Wang, Kun and Ma, Yunshan and Jie, Shi and Wang, Xiang and He, Xiangnan and Chua, Tat-seng},
	month = apr,
	year = {2025},
	note = {arXiv:2410.02355 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/JYLBSUYB/Fang et al. - 2025 - AlphaEdit Null-Space Constrained Knowledge Editing for Language Models.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/X7MBF4CM/2410.html:text/html},
}

@inproceedings{meng_locating_2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {17359--17372},
	file = {PDF:/Users/savasp/Zotero/storage/VZKJPWNX/Meng et al. - 2022 - Locating and Editing Factual Associations in GPT.pdf:application/pdf},
}

@misc{wang_wise_2024,
	title = {{WISE}: {Rethinking} the {Knowledge} {Memory} for {Lifelong} {Model} {Editing} of {Large} {Language} {Models}},
	shorttitle = {{WISE}},
	url = {http://arxiv.org/abs/2405.14768},
	doi = {10.48550/arXiv.2405.14768},
	abstract = {Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is available at https://github.com/zjunlp/EasyEdit.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Wang, Peng and Li, Zexi and Zhang, Ningyu and Xu, Ziwen and Yao, Yunzhi and Jiang, Yong and Xie, Pengjun and Huang, Fei and Chen, Huajun},
	month = dec,
	year = {2024},
	note = {arXiv:2405.14768 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/SX875KSJ/Wang et al. - 2024 - WISE Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/M4X4GWDJ/2405.html:text/html},
}

@inproceedings{zheng_can_2023,
	address = {Singapore},
	title = {Can {We} {Edit} {Factual} {Knowledge} by {In}-{Context} {Learning}?},
	url = {https://aclanthology.org/2023.emnlp-main.296/},
	doi = {10.18653/v1/2023.emnlp-main.296},
	abstract = {Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/pkunlp-icler/IKE.},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zheng, Ce and Li, Lei and Dong, Qingxiu and Fan, Yuxuan and Wu, Zhiyong and Xu, Jingjing and Chang, Baobao},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {4862--4876},
	file = {PDF:/Users/savasp/Zotero/storage/AVYBJ98Z/Zheng et al. - 2023 - Can We Edit Factual Knowledge by In-Context Learning.pdf:application/pdf},
}

@misc{wang_memory_2022,
	title = {Memory {Replay} with {Data} {Compression} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/2202.06592},
	doi = {10.48550/arXiv.2202.06592},
	abstract = {Continual learning needs to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing work is mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression (MRDC) to reduce the storage cost of old training samples and thus increase their amount that can be stored in the memory buffer. Observing that the trade-off between the quality and quantity of compressed data is highly nontrivial for the efficacy of memory replay, we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate compression quality for currently-arrived training samples. In this way, using a naive data compression algorithm with a properly selected quality can largely boost recent strong baselines by saving more compressed data in a limited storage space. We extensively validate this across several benchmarks of class-incremental learning and in a realistic scenario of object detection for autonomous driving.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Wang, Liyuan and Zhang, Xingxing and Yang, Kuo and Yu, Longhui and Li, Chongxuan and Hong, Lanqing and Zhang, Shifeng and Li, Zhenguo and Zhong, Yi and Zhu, Jun},
	month = mar,
	year = {2022},
	note = {arXiv:2202.06592 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/864LZEZR/Wang et al. - 2022 - Memory Replay with Data Compression for Continual Learning.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/JWR4ZZNX/2202.html:text/html},
}

@misc{wang_memoryllm_2024,
	title = {{MEMORYLLM}: {Towards} {Self}-{Updatable} {Large} {Language} {Models}},
	shorttitle = {{MEMORYLLM}},
	url = {http://arxiv.org/abs/2402.04624},
	doi = {10.48550/arXiv.2402.04624},
	abstract = {Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates. Our code and model are open-sourced at https://github.com/wangyu-ustc/MemoryLLM.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Wang, Yu and Gao, Yifan and Chen, Xiusi and Jiang, Haoming and Li, Shiyang and Yang, Jingfeng and Yin, Qingyu and Li, Zheng and Li, Xian and Yin, Bing and Shang, Jingbo and McAuley, Julian},
	month = may,
	year = {2024},
	note = {arXiv:2402.04624 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/FNEI7EAG/Wang et al. - 2024 - MEMORYLLM Towards Self-Updatable Large Language Models.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/4GPWFDTV/2402.html:text/html},
}

@inproceedings{driess_palm-e_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{PaLM}-{E}: {An} {Embodied} {Multimodal} {Language} {Model}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/driess23a.html},
	abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multimodal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {8469--8488},
}
