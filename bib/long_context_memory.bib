
@inproceedings{tai_improved_2015,
	address = {Beijing, China},
	title = {Improved {Semantic} {Representations} {From} {Tree}-{Structured} {Long} {Short}-{Term} {Memory} {Networks}},
	url = {https://aclanthology.org/P15-1150/},
	doi = {10.3115/v1/P15-1150},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	editor = {Zong, Chengqing and Strube, Michael},
	month = jul,
	year = {2015},
	pages = {1556--1566},
	file = {PDF:/Users/savasp/Zotero/storage/39BFZEHF/Tai et al. - 2015 - Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.pdf:application/pdf},
}

@misc{chen_walking_2023,
	title = {Walking {Down} the {Memory} {Maze}: {Beyond} {Context} {Limit} through {Interactive} {Reading}},
	shorttitle = {Walking {Down} the {Memory} {Maze}},
	url = {http://arxiv.org/abs/2310.05029},
	doi = {10.48550/arXiv.2310.05029},
	abstract = {Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Chen, Howard and Pasunuru, Ramakanth and Weston, Jason and Celikyilmaz, Asli},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05029 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/H5376BDZ/Chen et al. - 2023 - Walking Down the Memory Maze Beyond Context Limit through Interactive Reading.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/LI9JNBBR/2310.html:text/html},
}

@inproceedings{zhang_cam_2024,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{CaM}: {Cache} {Merging} for {Memory}-efficient {LLMs} {Inference}},
	volume = {235},
	url = {https://proceedings.mlr.press/v235/zhang24n.html},
	abstract = {Despite the exceptional performance of Large Language Models (LLMs), the substantial volume of key-value (KV) pairs cached during inference presents a barrier to their efficient deployment. To ameliorate this, recent works have aimed to selectively eliminate these caches, informed by the attention scores of associated tokens. However, such cache eviction invariably leads to output perturbation, regardless of the token choice. This perturbation escalates with the compression ratio, which can precipitate a marked deterioration in LLM inference performance. This paper introduces Cache Merging (CaM) as a solution to mitigate this challenge. CaM adaptively merges to-be-evicted caches into the remaining ones, employing a novel sampling strategy governed by the prominence of attention scores within discarded locations. In this manner, CaM enables memory-efficient LLMs to preserve critical token information, even obviating the need to maintain their corresponding caches. Extensive experiments utilizing LLaMA, OPT, and GPT-NeoX across various benchmarks corroborate CaM’s proficiency in bolstering the performance of memory-efficient LLMs. Code is released at https://github.com/zyxxmu/cam.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong},
	editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
	month = jul,
	year = {2024},
	pages = {58840--58850},
}

@misc{bulatov_recurrent_2022,
	title = {Recurrent {Memory} {Transformer}},
	url = {http://arxiv.org/abs/2207.06881},
	doi = {10.48550/arXiv.2207.06881},
	abstract = {Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S.},
	month = dec,
	year = {2022},
	note = {arXiv:2207.06881 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/8QEXZJN2/Bulatov et al. - 2022 - Recurrent Memory Transformer.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/6ARZI4HB/2207.html:text/html},
}

@misc{xiao_infllm_2024,
	title = {{InfLLM}: {Training}-{Free} {Long}-{Context} {Extrapolation} for {LLMs} with an {Efficient} {Context} {Memory}},
	url = {https://icml.cc/virtual/2024/39062},
	author = {Xiao, Chaojun},
	month = jul,
	year = {2024},
}

@inproceedings{lee_human-inspired_2024,
	address = {Vienna, Austria},
	series = {{ICML}'24},
	title = {A human-inspired reading agent with gist memory of very long contexts},
	abstract = {Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20× in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3.5 - 20×.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Lee, Kuang-Huei and Chen, Xinyun and Furuta, Hiroki and Canny, John and Fischer, Ian},
	year = {2024},
	file = {PDF:/Users/savasp/Zotero/storage/MFA35627/Lee et al. - 2024 - A human-inspired reading agent with gist memory of very long contexts.pdf:application/pdf},
}

@misc{zheng_synapse_2024,
	title = {Synapse: {Trajectory}-as-{Exemplar} {Prompting} with {Memory} for {Computer} {Control}},
	shorttitle = {Synapse},
	url = {http://arxiv.org/abs/2306.07863},
	doi = {10.48550/arXiv.2306.07863},
	abstract = {Building agents with large language models (LLMs) for computer control is a burgeoning research area, where the agent receives computer states and performs actions to complete complex tasks. Previous computer agents have demonstrated the benefits of in-context learning (ICL); however, their performance is hindered by several issues. First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exemplars and overlook the similarity among tasks, resulting in poor generalization to novel tasks. To address these challenges, we introduce Synapse, a computer agent featuring three key components: i) state abstraction, which filters out task-irrelevant information from raw states, allowing more exemplars within the limited context, ii) trajectory-as-exemplar prompting, which prompts the LLM with complete trajectories of the abstracted states and actions to improve multi-step decision-making, and iii) exemplar memory, which stores the embeddings of exemplars and retrieves them via similarity search for generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse achieves a 99.2\% average success rate (a 10\% relative improvement) across 64 tasks using demonstrations from only 48 tasks. Notably, Synapse is the first ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a 56\% relative improvement in average step success rate over the previous state-of-the-art prompting scheme in Mind2Web.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Zheng, Longtao and Wang, Rundong and Wang, Xinrun and An, Bo},
	month = jan,
	year = {2024},
	note = {arXiv:2306.07863 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/5F4YRXUV/Zheng et al. - 2024 - Synapse Trajectory-as-Exemplar Prompting with Memory for Computer Control.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/EGKA3EFH/2306.html:text/html},
}

@misc{samsami_mastering_2024,
	title = {Mastering {Memory} {Tasks} with {World} {Models}},
	url = {http://arxiv.org/abs/2403.04253},
	doi = {10.48550/arXiv.2403.04253},
	abstract = {Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggesting the generality of our method. We also show that R2I is faster than the state-of-the-art MBRL method, DreamerV3, resulting in faster wall-time convergence.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Samsami, Mohammad Reza and Zholus, Artem and Rajendran, Janarthanan and Chandar, Sarath},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04253 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/EBDWX25Y/Samsami et al. - 2024 - Mastering Memory Tasks with World Models.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/XW8FTVP2/2403.html:text/html},
}

@inproceedings{balazevic_memory_2024,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Memory {Consolidation} {Enables} {Long}-{Context} {Video} {Understanding}},
	volume = {235},
	url = {https://proceedings.mlr.press/v235/balazevic24a.html},
	abstract = {Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. We propose to instead re-purpose existing pre-trained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memory-consolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Balazevic, Ivana and Shi, Yuge and Papalampidi, Pinelopi and Chaabouni, Rahma and Koppula, Skanda and Henaff, Olivier J},
	editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
	month = jul,
	year = {2024},
	pages = {2527--2542},
}

@inproceedings{tack_online_2024,
	title = {Online {Adaptation} of {Language} {Models} with a {Memory} of {Amortized} {Contexts}},
	url = {https://openreview.net/forum?id=RIfgKCknTu},
	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Tack, Jihoon and Kim, Jaehyung and Mitchell, Eric and Shin, Jinwoo and Teh, Yee Whye and Schwarz, Jonathan Richard},
	year = {2024},
	file = {PDF:/Users/savasp/Zotero/storage/T86Q8AC4/Tack et al. - 2024 - Online Adaptation of Language Models with a Memory of Amortized Contexts.pdf:application/pdf},
}

@inproceedings{kim_infinipot_2024,
	address = {Miami, Florida, USA},
	title = {{InfiniPot}: {Infinite} {Context} {Processing} on {Memory}-{Constrained} {LLMs}},
	url = {https://aclanthology.org/2024.emnlp-main.897/},
	doi = {10.18653/v1/2024.emnlp-main.897},
	abstract = {Handling long input contexts remains a significant challenge for Large Language Models (LLMs), particularly in resource-constrained environments such as mobile devices. Our work aims to address this limitation by introducing InfiniPot, a novel KV cache control framework designed to enable pre-trained LLMs to manage extensive sequences within fixed memory constraints efficiently, without requiring additional training. InfiniPot leverages Continual Context Distillation (CCD), an iterative process that compresses and retains essential information through novel importance metrics, effectively maintaining critical data even without access to future context. Our comprehensive evaluations indicate that InfiniPot significantly outperforms models trained for long contexts in various NLP tasks, establishing its efficacy and versatility. This work represents a substantial advancement toward making LLMs applicable to a broader range of real-world scenarios.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Minsoo and Shim, Kyuhong and Choi, Jungwook and Chang, Simyung},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {16046--16060},
	file = {PDF:/Users/savasp/Zotero/storage/B8PQCLQ5/Kim et al. - 2024 - InfiniPot Infinite Context Processing on Memory-Constrained LLMs.pdf:application/pdf},
}

@inproceedings{zancato_bmojo_2024,
	title = {B'{MOJO}: {Hybrid} {State} {Space} {Realizations} of {Foundation} {Models} with {Eidetic} and {Fading} {Memory}},
	url = {https://neurips.cc/virtual/2024/poster/95153},
	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Zancato, Luca and Seshadri, Arjun and Dukler, Yonatan and Golatkar, Aditya and Shen, Yantao and Bowman, Benjamin and Trager, Matthew and Achille, Alessandro and Soatto, Stefano},
	year = {2024},
	file = {PDF:/Users/savasp/Zotero/storage/WYJ48UZQ/Zancato et al. - 2024 - B'MOJO Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory.pdf:application/pdf},
}

@article{nelson_needle_2024,
	title = {Needle in the {Haystack} for {Memory} {Based} {Large} {Language} {Models}},
	volume = {abs/2407.01437},
	url = {https://doi.org/10.48550/arXiv.2407.01437},
	journal = {CoRR},
	author = {Nelson, Elliot and Kollias, Georgios and Das, Payel and Chaudhury, Subhajit and Dan, Soham},
	year = {2024},
	file = {PDF:/Users/savasp/Zotero/storage/TFY8EA9Z/Nelson et al. - 2024 - Needle in the Haystack for Memory Based Large Language Models.pdf:application/pdf},
}

@inproceedings{zeng_memorize_2024,
	address = {Miami, Florida, USA},
	title = {Memorize {Step} by {Step}: {Efficient} {Long}-{Context} {Prefilling} with {Incremental} {Memory} and {Decremental} {Chunk}},
	url = {https://aclanthology.org/2024.emnlp-main.1169/},
	doi = {10.18653/v1/2024.emnlp-main.1169},
	abstract = {The evolution of Large Language Models (LLMs) has led to significant advancements, with models like Claude and Gemini capable of processing contexts up to 1 million tokens. However, efficiently handling long sequences remains challenging, particularly during the prefilling stage when input lengths exceed GPU memory capacity. Traditional methods often segment sequence into chunks and compress them iteratively with fixed-size memory. However, our empirical analysis shows that the fixed-size memory results in wasted computational and GPU memory resources. Therefore, we introduces Incremental Memory (IM), a method that starts with a small memory size and gradually increases it, optimizing computational efficiency. Additionally, we propose Decremental Chunk based on Incremental Memory (IMDC), which reduces chunk size while increasing memory size, ensuring stable and lower GPU memory usage. Our experiments demonstrate that IMDC is consistently faster (1.45x) and reduces GPU memory consumption by 23.3\% compared to fixed-size memory, achieving comparable performance on the LongBench Benchmark.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zeng, Zhiyuan and Guo, Qipeng and Liu, Xiaoran and Yin, Zhangyue and Shu, Wentao and Huang, Mianqiu and Wang, Bo and Zhou, Yunhua and Li, Linlin and Liu, Qun and Qiu, Xipeng},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {21021--21034},
	file = {PDF:/Users/savasp/Zotero/storage/PJHFXN3X/Zeng et al. - 2024 - Memorize Step by Step Efficient Long-Context Prefilling with Incremental Memory and Decremental Chu.pdf:application/pdf},
}

@misc{berges_memory_2024,
	title = {Memory {Layers} at {Scale}},
	url = {http://arxiv.org/abs/2412.09764},
	doi = {10.48550/arXiv.2412.09764},
	abstract = {Memory layers use a trainable key-value lookup mechanism to add extra parameters to a model without increasing FLOPs. Conceptually, sparsely activated memory layers complement compute-heavy dense feed-forward layers, providing dedicated capacity to store and retrieve information cheaply. This work takes memory layers beyond proof-of-concept, proving their utility at contemporary scale. On downstream tasks, language models augmented with our improved memory layer outperform dense models with more than twice the computation budget, as well as mixture-of-expert models when matched for both compute and parameters. We find gains are especially pronounced for factual tasks. We provide a fully parallelizable memory layer implementation, demonstrating scaling laws with up to 128B memory parameters, pretrained to 1 trillion tokens, comparing to base models with up to 8B parameters.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Berges, Vincent-Pierre and Oğuz, Barlas and Haziza, Daniel and Yih, Wen-tau and Zettlemoyer, Luke and Ghosh, Gargi},
	month = dec,
	year = {2024},
	note = {arXiv:2412.09764 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/HJSDK3AR/Berges et al. - 2024 - Memory Layers at Scale.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/5B9DGWMQ/2412.html:text/html},
}

@inproceedings{sakarvadia_memory_2023,
	address = {Singapore},
	title = {Memory {Injections}: {Correcting} {Multi}-{Hop} {Reasoning} {Failures} {During} {Inference} in {Transformer}-{Based} {Language} {Models}},
	url = {https://aclanthology.org/2023.blackboxnlp-1.26/},
	doi = {10.18653/v1/2023.blackboxnlp-1.26},
	abstract = {Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as “memories,” at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424\%.},
	booktitle = {Proceedings of the 6th {BlackboxNLP} {Workshop}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Sakarvadia, Mansi and Ajith, Aswathy and Khan, Arham and Grzenda, Daniel and Hudson, Nathaniel and Bauer, André and Chard, Kyle and Foster, Ian},
	editor = {Belinkov, Yonatan and Hao, Sophie and Jumelet, Jaap and Kim, Najoung and McCarthy, Arya and Mohebbi, Hosein},
	month = dec,
	year = {2023},
	pages = {342--356},
	file = {PDF:/Users/savasp/Zotero/storage/W4J8YCCC/Sakarvadia et al. - 2023 - Memory Injections Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Lan.pdf:application/pdf},
}

@inproceedings{wang_when_2024,
	address = {Miami, Florida, USA},
	title = {When {Compression} {Meets} {Model} {Compression}: {Memory}-{Efficient} {Double} {Compression} for {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.findings-emnlp.988/},
	doi = {10.18653/v1/2024.findings-emnlp.988},
	abstract = {Large language models (LLMs) exhibit excellent performance in various tasks. However, the memory requirements of LLMs present a great challenge when deploying on memory-limited devices, even for quantized LLMs. This paper introduces a framework to compress LLM after quantization further, achieving about 2.2x compression ratio. A compression-aware quantization is first proposed to enhance model weight compressibility by re-scaling the model parameters before quantization, followed by a pruning method to improve further. Upon this, we notice that decompression can be a bottleneck during practical scenarios. We then give a detailed analysis of the trade-off between memory usage and latency brought by the proposed method. A speed-adaptive method is proposed to overcome it. The experimental results show inference with the compressed model can achieve a 40\% reduction in memory size with negligible loss in accuracy and inference speed.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Weilan and Mao, Yu and Dongdong, Tang and Hongchao, Du and Guan, Nan and Xue, Chun Jason},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {16973--16983},
	file = {PDF:/Users/savasp/Zotero/storage/Q7H5MEI5/Wang et al. - 2024 - When Compression Meets Model Compression Memory-Efficient Double Compression for Large Language Mod.pdf:application/pdf},
}

@misc{chen_melodi_2024,
	title = {{MELODI}: {Exploring} {Memory} {Compression} for {Long} {Contexts}},
	shorttitle = {{MELODI}},
	url = {http://arxiv.org/abs/2410.03156},
	doi = {10.48550/arXiv.2410.03156},
	abstract = {We present MELODI, a novel memory architecture designed to efficiently process long documents using short context windows. The key principle behind MELODI is to represent short-term and long-term memory as a hierarchical compression scheme across both network layers and context windows. Specifically, the short-term memory is achieved through recurrent compression of context windows across multiple layers, ensuring smooth transitions between windows. In contrast, the long-term memory performs further compression within a single middle layer and aggregates information across context windows, effectively consolidating crucial information from the entire history. Compared to a strong baseline - the Memorizing Transformer employing dense attention over a large long-term memory (64K key-value pairs) - our method demonstrates superior performance on various long-context datasets while remarkably reducing the memory footprint by a factor of 8.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Chen, Yinpeng and Hutchins, DeLesley and Jansen, Aren and Zhmoginov, Andrey and Racz, David and Andersen, Jesper},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03156 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/GYEUZZ8F/Chen et al. - 2024 - MELODI Exploring Memory Compression for Long Contexts.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/399B6QNU/2410.html:text/html},
}

@inproceedings{chi_transformer_2023,
	address = {Singapore},
	title = {Transformer {Working} {Memory} {Enables} {Regular} {Language} {Reasoning} {And} {Natural} {Language} {Length} {Extrapolation}},
	url = {https://aclanthology.org/2023.findings-emnlp.397/},
	doi = {10.18653/v1/2023.findings-emnlp.397},
	abstract = {Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Chi, Ta-Chung and Fan, Ting-Han and Rudnicky, Alexander and Ramadge, Peter},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {5972--5984},
	file = {PDF:/Users/savasp/Zotero/storage/RFW42H86/Chi et al. - 2023 - Transformer Working Memory Enables Regular Language Reasoning And Natural Language Length Extrapolat.pdf:application/pdf},
}

@inproceedings{luo_mini-sequence_2024,
	title = {Mini-{Sequence} {Transformers}: {Optimizing} {Intermediate} {Memory} for {Long} {Sequences} {Training}},
	url = {https://neurips.cc/virtual/2024/poster/96824},
	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Luo, Cheng and Zhao, Jiawei and Chen, Zhuoming and Chen, Beidi and Anandkumar, Anima},
	year = {2024},
	file = {PDF:/Users/savasp/Zotero/storage/IK85VXCI/Luo et al. - 2024 - Mini-Sequence Transformers Optimizing Intermediate Memory for Long Sequences Training.pdf:application/pdf},
}

@inproceedings{feng_learn_2022,
	address = {Seattle, United States},
	title = {Learn {To} {Remember}: {Transformer} with {Recurrent} {Memory} for {Document}-{Level} {Machine} {Translation}},
	url = {https://aclanthology.org/2022.findings-naacl.105/},
	doi = {10.18653/v1/2022.findings-naacl.105},
	abstract = {The Transformer architecture has led to significant gains in machine translation. However, most studies focus on only sentence-level translation without considering the context dependency within documents, leading to the inadequacy of document-level coherence. Some recent research tried to mitigate this issue by introducing an additional context encoder or translating with multiple sentences or even the entire document. Such methods may lose the information on the target side or have an increasing computational complexity as documents get longer. To address such problems, we introduce a recurrent memory unit to the vanilla Transformer, which supports the information exchange between the sentence and previous context. The memory unit is recurrently updated by acquiring information from sentences, and passing the aggregated knowledge back to subsequent sentence states. We follow a two-stage training strategy, in which the model is first trained at the sentence level and then finetuned for document-level translation. We conduct experiments on three popular datasets for document-level machine translation and our model has an average improvement of 0.91 s-BLEU over the sentence-level baseline. We also achieve state-of-the-art results on TED and News, outperforming the previous work by 0.36 s-BLEU and 1.49 d-BLEU on average.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Yukun and Li, Feng and Song, Ziang and Zheng, Boyuan and Koehn, Philipp},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {1409--1420},
	file = {PDF:/Users/savasp/Zotero/storage/2UBFPF9Q/Feng et al. - 2022 - Learn To Remember Transformer with Recurrent Memory for Document-Level Machine Translation.pdf:application/pdf},
}

@inproceedings{zhang_linearizing_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Linearizing {Transformer} with {Key}-{Value} {Memory}},
	url = {https://aclanthology.org/2022.emnlp-main.24/},
	doi = {10.18653/v1/2022.emnlp-main.24},
	abstract = {Efficient transformer variants with linear time complexity have been developed to mitigate the quadratic computational overhead of the vanilla transformer. Among them are low-rank projection methods such as Linformer and kernel-based Transformers. Despite their unique merits, they usually suffer from a performance drop comparing with the vanilla transformer on many sequence generation tasks, and often fail to obtain computation gain when the generation is short. We propose Memsizer, an approach towards closing the performance gap while improving the efficiency even with short generation. It projects the source sequences into lower dimension representations like Linformer, while enjoying efficient recurrent-style incremental computation similar to kernel-based transformers. This yields linear computation time and constant memory complexity at inference time. Memsizer also employs a lightweight multi-head mechanism which renders the computation as light as a single-head model. We demonstrate that Memsizer provides an improved balance between efficiency and accuracy over the vanilla transformer and other efficient transformer variants in three typical sequence generation tasks, including machine translation, abstractive text summarization, and language modeling.},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yizhe and Cai, Deng},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {346--359},
	file = {PDF:/Users/savasp/Zotero/storage/W4K3ELQB/Zhang and Cai - 2022 - Linearizing Transformer with Key-Value Memory.pdf:application/pdf},
}

@inproceedings{araujo_memory_2023,
	address = {Toronto, Canada},
	title = {A {Memory} {Model} for {Question} {Answering} from {Streaming} {Data} {Supported} by {Rehearsal} and {Anticipation} of {Coreference} {Information}},
	url = {https://aclanthology.org/2023.findings-acl.830/},
	doi = {10.18653/v1/2023.findings-acl.830},
	abstract = {Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-sequence textual (NarrativeQA) and video (ActivityNet-QA) question answering datasets, where it achieves substantial improvements over previous memory network approaches. Furthermore, our ablation study confirms the proposed mechanisms' importance for memory models.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Araujo, Vladimir and Soto, Alvaro and Moens, Marie-Francine},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {13124--13138},
	file = {PDF:/Users/savasp/Zotero/storage/LGDSEBYN/Araujo et al. - 2023 - A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of.pdf:application/pdf},
}

@inproceedings{ji_lamemo_2022,
	address = {Seattle, United States},
	title = {{LaMemo}: {Language} {Modeling} with {Look}-{Ahead} {Memory}},
	url = {https://aclanthology.org/2022.naacl-main.422/},
	doi = {10.18653/v1/2022.naacl-main.422},
	abstract = {Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-to-date information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) that enhances the recurrence memory by incrementally attending to the right-side tokens and interpolating with the old memory states to maintain long-term information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory mechanisms.},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Ji, Haozhe and Zhang, Rongsheng and Yang, Zhenyu and Hu, Zhipeng and Huang, Minlie},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {5747--5762},
	file = {PDF:/Users/savasp/Zotero/storage/RTAWRQKD/Ji et al. - 2022 - LaMemo Language Modeling with Look-Ahead Memory.pdf:application/pdf},
}

@misc{cetin_evolved_2025,
	title = {An {Evolved} {Universal} {Transformer} {Memory}},
	url = {http://arxiv.org/abs/2410.13166},
	doi = {10.48550/arXiv.2410.13166},
	abstract = {Prior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance. We overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a learned network for memory management that improves both the performance and efficiency of transformers. We evolve NAMMs atop pre-trained transformers to provide different latent contexts focusing on the most relevant information for individual layers and attention heads. NAMMs are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices. Learning NAMMs on a small set of problems, we achieve substantial performance improvements across multiple long-context benchmarks while cutting the model's input contexts up to a fraction of the original sizes. We show the generality of our conditioning enables zero-shot transfer of NAMMs trained only on language to entirely new transformer architectures even across input modalities, with their benefits carrying over to vision and reinforcement learning.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Cetin, Edoardo and Sun, Qi and Zhao, Tianyu and Tang, Yujin},
	month = feb,
	year = {2025},
	note = {arXiv:2410.13166 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/savasp/Zotero/storage/6HV392Y4/Cetin et al. - 2025 - An Evolved Universal Transformer Memory.pdf:application/pdf;Snapshot:/Users/savasp/Zotero/storage/LXLJYQDG/2410.html:text/html},
}

@inproceedings{tao_when_2024,
	address = {Miami, Florida, USA},
	title = {When {Context} {Leads} but {Parametric} {Memory} {Follows} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.emnlp-main.234/},
	doi = {10.18653/v1/2024.emnlp-main.234},
	abstract = {Large language models (LLMs) have demonstrated remarkable progress in leveraging diverse knowledge sources. This study investigates how nine widely used LLMs allocate knowledge between local context and global parameters when answering open-ended questions in knowledge-consistent scenarios. We introduce a novel dataset, WikiAtomic, and systematically vary context sizes to analyze how LLMs prioritize and utilize the provided information and their parametric knowledge in knowledge-consistent scenarios. Additionally, we also study their tendency to hallucinate under varying context sizes. Our findings reveal consistent patterns across models, including a consistent reliance on both contextual (around 70\%) and parametric (around 30\%) knowledge, and a decrease in hallucinations with increasing context. These insights highlight the importance of more effective context organization and developing models that use input more deterministically for robust performance.},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Tao, Yufei and Hiatt, Adam and Haake, Erik and Jetter, Antonie J. and Agrawal, Ameeta},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {4034--4058},
	file = {PDF:/Users/savasp/Zotero/storage/5KSZMN8Z/Tao et al. - 2024 - When Context Leads but Parametric Memory Follows in Large Language Models.pdf:application/pdf},
}
